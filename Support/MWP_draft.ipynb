{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM\n",
    "    from keras.layers import CuDNNGRU as GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some config\n",
    "BATCH_SIZE = 64  # Batch size for training.\n",
    "EPOCHS = 100  # Number of epochs to train for.\n",
    "LATENT_DIM = 256  # Latent dimensionality of the encoding space.\n",
    "#NUM_SAMPLES = 10000  # Number of samples to train on (only when the data length is very high and we want to train on less)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 10000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"dolphin_t2_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alignment</th>\n",
       "      <th>Equiv</th>\n",
       "      <th>Template</th>\n",
       "      <th>iIndex</th>\n",
       "      <th>lEquations</th>\n",
       "      <th>lSolutions</th>\n",
       "      <th>sQuestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'coeff': 'a', 'SentenceId': 0, 'Value': 18.0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[m + n = a, m - n = b]</td>\n",
       "      <td>2000989</td>\n",
       "      <td>[x+y=18.0, x-y=4.0]</td>\n",
       "      <td>[11.0, 7.0]</td>\n",
       "      <td>The sum of two numbers is 18 and their differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'coeff': 'a', 'SentenceId': 0, 'Value': 22.0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[m + n = a, m - n = b]</td>\n",
       "      <td>2001344</td>\n",
       "      <td>[x+y=22.0, x-y=4.0]</td>\n",
       "      <td>[13.0, 9.0]</td>\n",
       "      <td>Two numbers have a sum of 22 . Their differenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'coeff': 'a', 'SentenceId': 0, 'Value': 34.0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[m + n = a, m - n = b]</td>\n",
       "      <td>2001019</td>\n",
       "      <td>[x-y=10.0, x+y=34.0]</td>\n",
       "      <td>[22.0, 12.0]</td>\n",
       "      <td>Two numbers have a difference of 10 and the su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'coeff': 'a', 'SentenceId': 0, 'Value': 38.0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[m + n = a, m - n = b]</td>\n",
       "      <td>2001024</td>\n",
       "      <td>[x+y=38.0, x-y=12.0]</td>\n",
       "      <td>[25.0, 13.0]</td>\n",
       "      <td>The sum of two numbers is 38 . Their differenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'coeff': 'a', 'SentenceId': 0, 'Value': 39.0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[m + n = a, m - n = b]</td>\n",
       "      <td>2001038</td>\n",
       "      <td>[x+y=39.0, x-y=7.0]</td>\n",
       "      <td>[23.0, 16.0]</td>\n",
       "      <td>The sum of two numbers is 39 and their differe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Alignment Equiv  \\\n",
       "0  [{'coeff': 'a', 'SentenceId': 0, 'Value': 18.0...    []   \n",
       "1  [{'coeff': 'a', 'SentenceId': 0, 'Value': 22.0...    []   \n",
       "2  [{'coeff': 'a', 'SentenceId': 0, 'Value': 34.0...    []   \n",
       "3  [{'coeff': 'a', 'SentenceId': 0, 'Value': 38.0...    []   \n",
       "4  [{'coeff': 'a', 'SentenceId': 0, 'Value': 39.0...    []   \n",
       "\n",
       "                 Template   iIndex            lEquations    lSolutions  \\\n",
       "0  [m + n = a, m - n = b]  2000989   [x+y=18.0, x-y=4.0]   [11.0, 7.0]   \n",
       "1  [m + n = a, m - n = b]  2001344   [x+y=22.0, x-y=4.0]   [13.0, 9.0]   \n",
       "2  [m + n = a, m - n = b]  2001019  [x-y=10.0, x+y=34.0]  [22.0, 12.0]   \n",
       "3  [m + n = a, m - n = b]  2001024  [x+y=38.0, x-y=12.0]  [25.0, 13.0]   \n",
       "4  [m + n = a, m - n = b]  2001038   [x+y=39.0, x-y=7.0]  [23.0, 16.0]   \n",
       "\n",
       "                                           sQuestion  \n",
       "0  The sum of two numbers is 18 and their differe...  \n",
       "1  Two numbers have a sum of 22 . Their differenc...  \n",
       "2  Two numbers have a difference of 10 and the su...  \n",
       "3  The sum of two numbers is 38 . Their differenc...  \n",
       "4  The sum of two numbers is 39 and their differe...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 831\n"
     ]
    }
   ],
   "source": [
    "print(\"num samples:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "--------\n",
      "Question:  find two consecutive even integers whose sum is 126\n",
      "Equation:  ['x+(x+2.0)=126.0']\n",
      "Solution:  [62.0]\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "print(\"Example:\")\n",
    "print(\"--------\")\n",
    "print(\"Question: \", data.iloc[110,6])\n",
    "print(\"Equation: \", data.iloc[110,4])\n",
    "print(\"Solution: \", data.iloc[110,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19\n",
       "1    19\n",
       "2    18\n",
       "3    19\n",
       "4    18\n",
       "Name: Question_Length, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quest_len(text):\n",
    "    split_text = text.split()\n",
    "    return len(split_text)\n",
    "\n",
    "data[\"Question_Length\"] = data[\"sQuestion\"].apply(quest_len)\n",
    "data[\"Question_Length\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21ff45a5f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"Question_Length\"].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # determine maximum length input sequence\n",
    "# max_len_input = max(len(s) for s in input_sequences)\n",
    "# max_len_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I am not taking maximun, to avoid wasted calsulation (knowing that any question beyond 33 will get false prediction)\n",
    "max_len_input = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to store the input (questions), the target (equations with EOS) and the target input (equations with SOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sum of two numbers is 18 and their difference is 4 . What are the two numbers ?',\n",
       " 'Two numbers have a sum of 22 . Their difference is 4 . what are the two numbers ?',\n",
       " 'Two numbers have a difference of 10 and the sum of 34 . What are the numbers ?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we store the questions\n",
    "input_texts = list(data[\"sQuestion\"])\n",
    "input_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     <sos> ['x+y=18.0', 'x-y=4.0']\n",
       "1     <sos> ['x+y=22.0', 'x-y=4.0']\n",
       "2    <sos> ['x-y=10.0', 'x+y=34.0']\n",
       "3    <sos> ['x+y=38.0', 'x-y=12.0']\n",
       "4     <sos> ['x+y=39.0', 'x-y=7.0']\n",
       "Name: lEquations_sos, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"lEquations_sos\"]  = '<sos> ' + data[\"lEquations\"].astype(str)\n",
    "data[\"lEquations_sos\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['x+y=18.0', 'x-y=4.0'] <eos>\n",
       "1     ['x+y=22.0', 'x-y=4.0'] <eos>\n",
       "2    ['x-y=10.0', 'x+y=34.0'] <eos>\n",
       "3    ['x+y=38.0', 'x-y=12.0'] <eos>\n",
       "4     ['x+y=39.0', 'x-y=7.0'] <eos>\n",
       "Name: lEquations_eos, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"lEquations_eos\"]  = data[\"lEquations\"].astype(str) + \" <eos>\"\n",
    "data[\"lEquations_eos\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_texts = list(data[\"lEquations_eos\"]) # sentence in target language\n",
    "target_texts_inputs = list(data[\"lEquations_sos\"]) # sentence in target language offset by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6, 3, 10, 12, 2, 89, 15, 47, 39, 2, 36, 14, 25, 1, 10, 12],\n",
       " [10, 12, 63, 23, 6, 3, 119, 47, 39, 2, 36, 14, 25, 1, 10, 12],\n",
       " [10, 12, 63, 23, 39, 3, 58, 15, 1, 6, 3, 120, 14, 25, 1, 12],\n",
       " [1, 6, 3, 10, 12, 2, 143, 47, 39, 2, 59, 14, 25, 1, 10, 12],\n",
       " [1, 6, 3, 10, 12, 2, 121, 15, 47, 39, 2, 54, 14, 25, 1, 12]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the input sequences\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each **input** word receives a designated index. For example '1' is 'the'; '10' is 'two' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 542 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "# get the word to index mapping for input language (dictionary of each word and its frequency)\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'re\": 254,\n",
       " \"'s\": 186,\n",
       " '0': 79,\n",
       " '00': 503,\n",
       " '000': 237,\n",
       " '038': 435,\n",
       " '1': 68,\n",
       " '10': 58,\n",
       " '100': 135,\n",
       " '101': 464,\n",
       " '105': 217,\n",
       " '107': 510,\n",
       " '108': 528,\n",
       " '109': 327,\n",
       " '11': 97,\n",
       " '110': 384,\n",
       " '111': 223,\n",
       " '112': 215,\n",
       " '114': 187,\n",
       " '116': 518,\n",
       " '117': 507,\n",
       " '118': 445,\n",
       " '119': 280,\n",
       " '12': 59,\n",
       " '120': 225,\n",
       " '122': 408,\n",
       " '123': 228,\n",
       " '1236': 468,\n",
       " '124': 192,\n",
       " '126': 265,\n",
       " '127': 359,\n",
       " '128': 526,\n",
       " '129': 337,\n",
       " '13': 104,\n",
       " '130': 426,\n",
       " '133': 334,\n",
       " '134': 437,\n",
       " '135': 302,\n",
       " '137': 200,\n",
       " '138': 502,\n",
       " '14': 153,\n",
       " '140': 537,\n",
       " '1405': 330,\n",
       " '144': 226,\n",
       " '145': 329,\n",
       " '1457': 442,\n",
       " '147': 418,\n",
       " '148': 517,\n",
       " '149': 446,\n",
       " '15': 80,\n",
       " '150': 257,\n",
       " '152': 501,\n",
       " '1524': 388,\n",
       " '153': 322,\n",
       " '156': 406,\n",
       " '157': 467,\n",
       " '16': 91,\n",
       " '162': 541,\n",
       " '164': 268,\n",
       " '16666666': 458,\n",
       " '168': 540,\n",
       " '17': 115,\n",
       " '171': 271,\n",
       " '174': 319,\n",
       " '176': 266,\n",
       " '18': 89,\n",
       " '182': 465,\n",
       " '185': 293,\n",
       " '186': 240,\n",
       " '188': 450,\n",
       " '19': 208,\n",
       " '192': 282,\n",
       " '194': 423,\n",
       " '195': 443,\n",
       " '199': 377,\n",
       " '1st': 233,\n",
       " '2': 43,\n",
       " '20': 83,\n",
       " '200': 312,\n",
       " '2001': 403,\n",
       " '2005': 398,\n",
       " '201': 376,\n",
       " '203': 378,\n",
       " '21': 147,\n",
       " '216': 224,\n",
       " '218': 424,\n",
       " '22': 119,\n",
       " '220': 275,\n",
       " '222': 381,\n",
       " '223': 417,\n",
       " '227': 405,\n",
       " '228': 382,\n",
       " '23': 105,\n",
       " '231': 454,\n",
       " '234': 166,\n",
       " '236': 396,\n",
       " '237': 182,\n",
       " '24': 86,\n",
       " '240': 262,\n",
       " '246': 386,\n",
       " '249': 221,\n",
       " '25': 99,\n",
       " '250': 404,\n",
       " '251': 438,\n",
       " '26': 189,\n",
       " '260': 527,\n",
       " '264': 451,\n",
       " '267': 320,\n",
       " '27': 113,\n",
       " '270': 389,\n",
       " '273': 230,\n",
       " '277': 416,\n",
       " '279': 515,\n",
       " '28': 125,\n",
       " '287': 415,\n",
       " '29': 122,\n",
       " '291': 350,\n",
       " '298': 391,\n",
       " '2nd': 179,\n",
       " '3': 29,\n",
       " '30': 76,\n",
       " '303': 394,\n",
       " '306': 481,\n",
       " '31': 210,\n",
       " '315': 364,\n",
       " '32': 100,\n",
       " '33': 106,\n",
       " '330': 509,\n",
       " '333333': 513,\n",
       " '336': 185,\n",
       " '34': 120,\n",
       " '340': 279,\n",
       " '344': 519,\n",
       " '35': 123,\n",
       " '357': 516,\n",
       " '36': 93,\n",
       " '360': 274,\n",
       " '365': 490,\n",
       " '37': 234,\n",
       " '38': 143,\n",
       " '383': 357,\n",
       " '388888': 368,\n",
       " '39': 121,\n",
       " '3rd': 116,\n",
       " '3w': 315,\n",
       " '3y': 285,\n",
       " '4': 36,\n",
       " '40': 110,\n",
       " '401b': 400,\n",
       " '403c': 401,\n",
       " '405d': 402,\n",
       " '41': 235,\n",
       " '416': 365,\n",
       " '417': 276,\n",
       " '42': 84,\n",
       " '420': 427,\n",
       " '43': 194,\n",
       " '434': 367,\n",
       " '44': 291,\n",
       " '449': 349,\n",
       " '45': 201,\n",
       " '46': 134,\n",
       " '468': 449,\n",
       " '47': 165,\n",
       " '48': 92,\n",
       " '480': 512,\n",
       " '49': 286,\n",
       " '490': 539,\n",
       " '4th': 393,\n",
       " '4y': 284,\n",
       " '5': 41,\n",
       " '50': 112,\n",
       " '500': 480,\n",
       " '51': 529,\n",
       " '52': 232,\n",
       " '53': 173,\n",
       " '54': 108,\n",
       " '547': 355,\n",
       " '55': 171,\n",
       " '55555555': 486,\n",
       " '56': 85,\n",
       " '57': 136,\n",
       " '574': 521,\n",
       " '58': 162,\n",
       " '59': 269,\n",
       " '6': 51,\n",
       " '60': 124,\n",
       " '61': 239,\n",
       " '62': 160,\n",
       " '621': 326,\n",
       " '63': 188,\n",
       " '64': 157,\n",
       " '640': 314,\n",
       " '65': 290,\n",
       " '66': 301,\n",
       " '67': 219,\n",
       " '68': 193,\n",
       " '680': 469,\n",
       " '69': 190,\n",
       " '7': 54,\n",
       " '70': 172,\n",
       " '71': 183,\n",
       " '72': 127,\n",
       " '720': 478,\n",
       " '73': 324,\n",
       " '74': 241,\n",
       " '75': 90,\n",
       " '753': 505,\n",
       " '76': 175,\n",
       " '770': 372,\n",
       " '78': 159,\n",
       " '79': 380,\n",
       " '8': 52,\n",
       " '80': 142,\n",
       " '804': 448,\n",
       " '81': 212,\n",
       " '82': 338,\n",
       " '825': 331,\n",
       " '83': 264,\n",
       " '84': 218,\n",
       " '85': 155,\n",
       " '86': 425,\n",
       " '87': 184,\n",
       " '875': 370,\n",
       " '88': 181,\n",
       " '89': 340,\n",
       " '9': 62,\n",
       " '90': 114,\n",
       " '91': 508,\n",
       " '93': 439,\n",
       " '94': 213,\n",
       " '95': 356,\n",
       " '96': 220,\n",
       " '960': 428,\n",
       " '97': 506,\n",
       " '98': 447,\n",
       " '99': 278,\n",
       " 'a': 23,\n",
       " 'actual': 373,\n",
       " 'add': 152,\n",
       " 'added': 49,\n",
       " 'adding': 379,\n",
       " 'adds': 245,\n",
       " 'after': 419,\n",
       " 'ages': 303,\n",
       " 'algebraically': 429,\n",
       " 'all': 231,\n",
       " 'also': 288,\n",
       " 'among': 407,\n",
       " 'an': 75,\n",
       " 'and': 15,\n",
       " 'another': 50,\n",
       " 'answer': 170,\n",
       " 'answers': 360,\n",
       " 'are': 25,\n",
       " 'arithmetic': 196,\n",
       " 'as': 53,\n",
       " 'athletes': 411,\n",
       " 'b': 158,\n",
       " 'be': 249,\n",
       " 'becomes': 538,\n",
       " 'begin': 420,\n",
       " 'between': 67,\n",
       " 'bigger': 292,\n",
       " 'book': 348,\n",
       " 'both': 242,\n",
       " 'boxes': 354,\n",
       " 'by': 38,\n",
       " 'c': 294,\n",
       " 'can': 317,\n",
       " 'certain': 202,\n",
       " 'consecutive': 8,\n",
       " 'consecutives': 525,\n",
       " 'cosecutive': 504,\n",
       " 'count': 421,\n",
       " 'counting': 422,\n",
       " 'countries': 412,\n",
       " 'country': 414,\n",
       " 'd': 295,\n",
       " 'days': 431,\n",
       " 'decreased': 81,\n",
       " 'describes': 375,\n",
       " 'determine': 229,\n",
       " 'differ': 466,\n",
       " 'difference': 39,\n",
       " 'digit': 111,\n",
       " 'digits': 82,\n",
       " 'diminished': 161,\n",
       " 'divided': 126,\n",
       " 'dividing': 332,\n",
       " 'do': 298,\n",
       " 'double': 300,\n",
       " 'doubled': 163,\n",
       " 'each': 131,\n",
       " 'eight': 73,\n",
       " 'eighteen': 520,\n",
       " 'eighty': 277,\n",
       " 'eleven': 174,\n",
       " 'end': 532,\n",
       " 'equal': 55,\n",
       " 'equals': 48,\n",
       " 'equation': 151,\n",
       " 'even': 30,\n",
       " 'every': 489,\n",
       " 'exceeds': 70,\n",
       " 'f': 297,\n",
       " 'fifteen': 214,\n",
       " 'fifth': 132,\n",
       " 'fifty': 392,\n",
       " 'figure': 483,\n",
       " 'find': 7,\n",
       " 'finds': 536,\n",
       " 'fine': 358,\n",
       " 'first': 18,\n",
       " 'fits': 363,\n",
       " 'five': 40,\n",
       " 'following': 383,\n",
       " 'for': 164,\n",
       " 'formed': 475,\n",
       " 'forty': 390,\n",
       " 'found': 336,\n",
       " 'four': 32,\n",
       " 'fourteen': 261,\n",
       " 'fourth': 96,\n",
       " 'fraction': 434,\n",
       " 'frida': 534,\n",
       " 'from': 61,\n",
       " 'gets': 462,\n",
       " 'give': 444,\n",
       " 'given': 216,\n",
       " 'gives': 146,\n",
       " 'got': 371,\n",
       " 'grant': 341,\n",
       " 'great': 287,\n",
       " 'greater': 44,\n",
       " 'greatest': 69,\n",
       " 'half': 129,\n",
       " 'has': 203,\n",
       " 'have': 63,\n",
       " 'he': 178,\n",
       " 'her': 253,\n",
       " 'highest': 395,\n",
       " 'his': 307,\n",
       " 'how': 169,\n",
       " 'hundred': 374,\n",
       " 'hundreds': 496,\n",
       " 'i': 95,\n",
       " 'if': 31,\n",
       " 'in': 139,\n",
       " 'inclusive': 488,\n",
       " 'increased': 94,\n",
       " 'increases': 473,\n",
       " 'increasing': 522,\n",
       " 'instead': 369,\n",
       " 'integer': 21,\n",
       " 'integers': 4,\n",
       " 'interchanged': 321,\n",
       " 'interchanging': 476,\n",
       " 'into': 258,\n",
       " 'is': 2,\n",
       " 'it': 167,\n",
       " 'its': 133,\n",
       " 'itself': 296,\n",
       " 'lager': 472,\n",
       " 'large': 141,\n",
       " 'larger': 27,\n",
       " 'largest': 46,\n",
       " 'last': 198,\n",
       " 'least': 78,\n",
       " 'left': 345,\n",
       " 'less': 26,\n",
       " 'lesser': 72,\n",
       " 'let': 524,\n",
       " 'letting': 270,\n",
       " 'list': 267,\n",
       " 'lrb': 168,\n",
       " 'm': 325,\n",
       " 'made': 494,\n",
       " 'make': 195,\n",
       " 'makes': 511,\n",
       " 'many': 316,\n",
       " 'mean': 385,\n",
       " 'medals': 409,\n",
       " 'metals': 413,\n",
       " 'middle': 103,\n",
       " 'minus': 88,\n",
       " 'more': 20,\n",
       " 'mulitples': 460,\n",
       " 'multiples': 463,\n",
       " 'multiplied': 227,\n",
       " 'multiply': 252,\n",
       " 'multiplyed': 485,\n",
       " 'my': 309,\n",
       " 'n': 156,\n",
       " 'need': 482,\n",
       " 'negative': 244,\n",
       " 'next': 128,\n",
       " 'nine': 109,\n",
       " 'no': 206,\n",
       " 'not': 499,\n",
       " 'number': 5,\n",
       " 'numbers': 12,\n",
       " 'numbersa': 399,\n",
       " 'obtained': 440,\n",
       " 'odd': 16,\n",
       " 'of': 3,\n",
       " 'office': 353,\n",
       " 'old': 455,\n",
       " 'oldest': 456,\n",
       " 'on': 351,\n",
       " 'one': 34,\n",
       " 'only': 474,\n",
       " 'open': 347,\n",
       " 'opposite': 148,\n",
       " 'or': 479,\n",
       " 'order': 523,\n",
       " 'original': 138,\n",
       " 'other': 56,\n",
       " 'out': 484,\n",
       " 'page': 263,\n",
       " 'part': 260,\n",
       " 'parts': 259,\n",
       " 'percent': 248,\n",
       " 'percentage': 311,\n",
       " 'permitted': 500,\n",
       " 'phil': 335,\n",
       " 'plus': 87,\n",
       " 'positive': 154,\n",
       " 'post': 352,\n",
       " 'prime': 497,\n",
       " 'problem': 397,\n",
       " 'product': 117,\n",
       " 'quantity': 310,\n",
       " 'quarter': 542,\n",
       " 'quotient': 176,\n",
       " 'ratio': 430,\n",
       " 'reciprocal': 433,\n",
       " 'remainder': 256,\n",
       " 'repeated': 495,\n",
       " 'repetition': 498,\n",
       " 'represent': 222,\n",
       " 'represents': 255,\n",
       " 'result': 42,\n",
       " 'results': 304,\n",
       " 'reversal': 477,\n",
       " 'reversed': 204,\n",
       " 'right': 346,\n",
       " 'robert': 306,\n",
       " 'rosenberg': 452,\n",
       " 'rrb': 118,\n",
       " 'same': 64,\n",
       " 'says': 342,\n",
       " 'second': 28,\n",
       " 'secret': 247,\n",
       " 'separate': 339,\n",
       " 'seperate': 343,\n",
       " 'sequence': 243,\n",
       " 'sequencey': 283,\n",
       " 'set': 362,\n",
       " 'seven': 71,\n",
       " 'she': 535,\n",
       " 'sister': 457,\n",
       " 'sisters': 453,\n",
       " 'situation': 273,\n",
       " 'six': 57,\n",
       " 'sixteen': 308,\n",
       " 'sixths': 487,\n",
       " 'size': 471,\n",
       " 'smaller': 22,\n",
       " 'smallest': 45,\n",
       " 'so': 98,\n",
       " 'solve': 238,\n",
       " 'some': 441,\n",
       " 'started': 533,\n",
       " 'subtract': 514,\n",
       " 'subtracted': 66,\n",
       " 'subtracts': 461,\n",
       " 'such': 24,\n",
       " 'suck': 387,\n",
       " 'sum': 6,\n",
       " 'ten': 102,\n",
       " 'terms': 318,\n",
       " 'than': 13,\n",
       " 'that': 19,\n",
       " 'the': 1,\n",
       " 'their': 47,\n",
       " 'them': 207,\n",
       " 'then': 130,\n",
       " 'there': 107,\n",
       " 'these': 77,\n",
       " 'they': 211,\n",
       " 'think': 251,\n",
       " 'thinking': 177,\n",
       " 'thinks': 459,\n",
       " 'third': 35,\n",
       " 'thirteen': 344,\n",
       " 'thirty': 272,\n",
       " 'this': 149,\n",
       " 'those': 246,\n",
       " 'three': 11,\n",
       " 'thrice': 289,\n",
       " 'time': 236,\n",
       " 'times': 9,\n",
       " 'to': 37,\n",
       " 'together': 492,\n",
       " 'total': 144,\n",
       " 'totaled': 366,\n",
       " 'totals': 530,\n",
       " 'triple': 281,\n",
       " 'tripled': 305,\n",
       " 'twelve': 199,\n",
       " 'twenty': 180,\n",
       " 'twice': 17,\n",
       " 'two': 10,\n",
       " 'twos': 436,\n",
       " 'up': 145,\n",
       " 'using': 250,\n",
       " 'value': 74,\n",
       " 'w': 205,\n",
       " 'was': 323,\n",
       " 'weeks': 432,\n",
       " 'were': 491,\n",
       " 'what': 14,\n",
       " 'whats': 470,\n",
       " 'when': 60,\n",
       " 'where': 197,\n",
       " 'which': 150,\n",
       " 'who': 361,\n",
       " 'whole': 137,\n",
       " 'whos': 299,\n",
       " 'whose': 33,\n",
       " 'will': 531,\n",
       " 'with': 101,\n",
       " 'won': 410,\n",
       " 'would': 493,\n",
       " 'write': 191,\n",
       " 'x': 65,\n",
       " 'y': 209,\n",
       " 'yeilds': 333,\n",
       " 'you': 140,\n",
       " 'z': 313,\n",
       " 'zero': 328}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining word_index\n",
    "word2idx_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # tokenize the outputs\n",
    "# # don't filter out special characters\n",
    "# # otherwise <sos> and <eos> won't appear\n",
    "# tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='[]')\n",
    "# tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) \n",
    "# target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "# target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examining the target sequences\n",
    "# target_sequences_inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_outputs.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !!! Currently takes as input only the train_data for the vocabulary!!!\n",
    "def create_counter(clean_df):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to crate a word counter for a corpus for 1. adjust the desired vocabulary size (see \n",
    "    vocab_coverage function), 2. to create a word index for padding (see create_word_index function)\n",
    "    input:\n",
    "    - clean_df: a pandas dataframe with a column of clean text (after initial preprocessing)\n",
    "    The function returns:\n",
    "    - lexicon: a list of all the tokens in the corpus\n",
    "    - word_count: a dictionary with all the tokens and their frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # First create the full lexicon (all the words/symbols form the new_text colomn)\n",
    "    lexicon = []\n",
    "    for sent in clean_df[\"lEquations_eos\"].values:\n",
    "        token = wordpunct_tokenize(sent) # Tokenizes each sentence\n",
    "        lexicon += token # and adds it to the lexicon\n",
    "    print(\"Full lexicon length: {}.\".format(len(lexicon)))\n",
    "    # Create a Counter object, which is a dictionary of all the words in the lexicon, and their frequency\n",
    "    word_counts = Counter(lexicon) \n",
    "    print(\"Counter (unique words) length: {}.\".format(len(word_counts)))\n",
    "    # I need to return both the lexicon and the word_count, since I use then for reducing the vocabulary\n",
    "    return lexicon, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full lexicon length: 22856.\n",
      "Counter (unique words) length: 276.\n"
     ]
    }
   ],
   "source": [
    "# Creating the lexicon and the counter\n",
    "lexicon, word_counter = create_counter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_index(lb, ub, counter):\n",
    "    \"\"\"\n",
    "    The function creates a word index, where each token is designated with an index. This is used for encoding.\n",
    "    input:\n",
    "    - lb: lower bound, the minimum number that a word appears in the original korpus\n",
    "    - ub: upper bound, the maximum number that a word appears in the original korpus\n",
    "    - counter: a Counter object that contains the frequency of each word \n",
    "    The function returns a dictionary with the words as keys and the index as values \n",
    "    \"\"\"\n",
    "    \n",
    "    lexicon_2 = [w for w in counter if ub >= counter[w] >= lb]\n",
    "    print(\"New lexicon length: {}.\".format(len(lexicon_2)))\n",
    "    word_index = {}\n",
    "    counter = 1\n",
    "    for word in lexicon_2:\n",
    "        word_index[word] = counter\n",
    "        counter += 1\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lexicon length: 276.\n"
     ]
    }
   ],
   "source": [
    "nltk_word_index = create_word_index(0, 5000, word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\": 10,\n",
       " \"'(\": 168,\n",
       " \"',\": 9,\n",
       " \"']\": 13,\n",
       " '(': 218,\n",
       " '(-': 86,\n",
       " ')': 217,\n",
       " \")',\": 36,\n",
       " \")']\": 87,\n",
       " ')*': 144,\n",
       " ')*(': 268,\n",
       " ')+': 70,\n",
       " ')+(': 69,\n",
       " ')-': 73,\n",
       " ')=': 72,\n",
       " ')=(': 68,\n",
       " ')=-': 74,\n",
       " '*': 66,\n",
       " '*(': 65,\n",
       " '+': 3,\n",
       " '+(': 105,\n",
       " '+(-': 34,\n",
       " '-': 11,\n",
       " '.': 7,\n",
       " '0': 8,\n",
       " '01': 193,\n",
       " '038': 209,\n",
       " '08': 250,\n",
       " '1': 52,\n",
       " '10': 18,\n",
       " '100': 96,\n",
       " '1000': 182,\n",
       " '101': 230,\n",
       " '105': 104,\n",
       " '107': 257,\n",
       " '108': 271,\n",
       " '109': 28,\n",
       " '11': 37,\n",
       " '110': 167,\n",
       " '111': 128,\n",
       " '112': 94,\n",
       " '114': 127,\n",
       " '116': 264,\n",
       " '117': 254,\n",
       " '118': 219,\n",
       " '119': 159,\n",
       " '12': 21,\n",
       " '120': 132,\n",
       " '122': 189,\n",
       " '123': 146,\n",
       " '1236': 233,\n",
       " '124': 153,\n",
       " '126': 113,\n",
       " '127': 118,\n",
       " '128': 269,\n",
       " '129': 92,\n",
       " '13': 32,\n",
       " '130': 201,\n",
       " '133': 89,\n",
       " '134': 211,\n",
       " '135': 227,\n",
       " '137': 202,\n",
       " '138': 246,\n",
       " '14': 97,\n",
       " '140': 273,\n",
       " '1405': 60,\n",
       " '144': 134,\n",
       " '145': 53,\n",
       " '1457': 215,\n",
       " '147': 195,\n",
       " '148': 263,\n",
       " '149': 220,\n",
       " '15': 83,\n",
       " '150': 90,\n",
       " '152': 247,\n",
       " '1524': 171,\n",
       " '153': 258,\n",
       " '156': 187,\n",
       " '157': 232,\n",
       " '16': 48,\n",
       " '164': 117,\n",
       " '16666666': 229,\n",
       " '168': 276,\n",
       " '17': 85,\n",
       " '171': 126,\n",
       " '174': 248,\n",
       " '176': 115,\n",
       " '18': 6,\n",
       " '182': 231,\n",
       " '183': 147,\n",
       " '185': 183,\n",
       " '186': 194,\n",
       " '188': 225,\n",
       " '19': 33,\n",
       " '192': 164,\n",
       " '194': 197,\n",
       " '195': 216,\n",
       " '199': 155,\n",
       " '2': 38,\n",
       " '20': 24,\n",
       " '200': 274,\n",
       " '2000': 249,\n",
       " '2005': 184,\n",
       " '201': 150,\n",
       " '203': 158,\n",
       " '21': 88,\n",
       " '216': 131,\n",
       " '218': 198,\n",
       " '22': 17,\n",
       " '220': 143,\n",
       " '222': 162,\n",
       " '223': 192,\n",
       " '227': 186,\n",
       " '228': 163,\n",
       " '23': 101,\n",
       " '231': 228,\n",
       " '234': 161,\n",
       " '236': 180,\n",
       " '237': 106,\n",
       " '24': 63,\n",
       " '240': 98,\n",
       " '246': 169,\n",
       " '249': 124,\n",
       " '25': 77,\n",
       " '250': 185,\n",
       " '251': 212,\n",
       " '26': 137,\n",
       " '260': 270,\n",
       " '264': 226,\n",
       " '267': 251,\n",
       " '27': 51,\n",
       " '270': 172,\n",
       " '273': 149,\n",
       " '277': 191,\n",
       " '279': 261,\n",
       " '28': 55,\n",
       " '287': 190,\n",
       " '29': 26,\n",
       " '291': 109,\n",
       " '298': 176,\n",
       " '3': 44,\n",
       " '30': 45,\n",
       " '303': 179,\n",
       " '306000': 238,\n",
       " '31': 56,\n",
       " '315': 129,\n",
       " '32': 79,\n",
       " '33': 103,\n",
       " '330': 256,\n",
       " '333333': 260,\n",
       " '333333333333': 139,\n",
       " '336': 125,\n",
       " '34': 19,\n",
       " '340': 152,\n",
       " '344': 265,\n",
       " '35': 47,\n",
       " '357': 262,\n",
       " '36': 80,\n",
       " '360': 135,\n",
       " '365': 244,\n",
       " '37': 177,\n",
       " '38': 20,\n",
       " '383': 112,\n",
       " '388888': 140,\n",
       " '39': 22,\n",
       " '4': 12,\n",
       " '40': 166,\n",
       " '41': 178,\n",
       " '416': 133,\n",
       " '417': 145,\n",
       " '42': 41,\n",
       " '420': 204,\n",
       " '43': 157,\n",
       " '434': 136,\n",
       " '44': 175,\n",
       " '449': 107,\n",
       " '45': 203,\n",
       " '46': 64,\n",
       " '468': 223,\n",
       " '47': 156,\n",
       " '48': 62,\n",
       " '480': 259,\n",
       " '49': 170,\n",
       " '490': 275,\n",
       " '5': 25,\n",
       " '50': 46,\n",
       " '500': 236,\n",
       " '51': 272,\n",
       " '52': 173,\n",
       " '53': 50,\n",
       " '54': 165,\n",
       " '547': 110,\n",
       " '55': 31,\n",
       " '55555555': 243,\n",
       " '56': 54,\n",
       " '57': 99,\n",
       " '574': 266,\n",
       " '58': 121,\n",
       " '59': 119,\n",
       " '6': 35,\n",
       " '60': 49,\n",
       " '61': 188,\n",
       " '62': 100,\n",
       " '621': 27,\n",
       " '63': 130,\n",
       " '64': 40,\n",
       " '640': 241,\n",
       " '65': 174,\n",
       " '66': 224,\n",
       " '67': 120,\n",
       " '68': 154,\n",
       " '680': 234,\n",
       " '69': 148,\n",
       " '7': 23,\n",
       " '70': 42,\n",
       " '71': 114,\n",
       " '72': 91,\n",
       " '720': 235,\n",
       " '73': 267,\n",
       " '74': 199,\n",
       " '75': 43,\n",
       " '753': 252,\n",
       " '76': 76,\n",
       " '770': 142,\n",
       " '78': 78,\n",
       " '79': 160,\n",
       " '8': 39,\n",
       " '80': 214,\n",
       " '804': 222,\n",
       " '81': 75,\n",
       " '82': 93,\n",
       " '8200': 239,\n",
       " '825': 82,\n",
       " '83': 108,\n",
       " '84': 116,\n",
       " '85': 196,\n",
       " '86': 200,\n",
       " '87': 122,\n",
       " '875': 141,\n",
       " '88': 102,\n",
       " '89': 95,\n",
       " '9': 57,\n",
       " '90': 61,\n",
       " '91': 255,\n",
       " '93': 213,\n",
       " '94': 81,\n",
       " '95': 111,\n",
       " '96': 123,\n",
       " '960': 205,\n",
       " '97': 253,\n",
       " '98': 221,\n",
       " '99': 151,\n",
       " '<': 14,\n",
       " '=': 5,\n",
       " '=(': 138,\n",
       " '=-': 181,\n",
       " '>': 16,\n",
       " \"['\": 1,\n",
       " \"['(\": 71,\n",
       " \"['-\": 84,\n",
       " 'a': 58,\n",
       " 'b': 59,\n",
       " 'd': 245,\n",
       " 'eos': 15,\n",
       " 'f': 206,\n",
       " 'g': 207,\n",
       " 'k': 67,\n",
       " 'm': 29,\n",
       " 'n': 30,\n",
       " 'p': 240,\n",
       " 'q': 237,\n",
       " 's': 208,\n",
       " 'w': 242,\n",
       " 'x': 2,\n",
       " 'y': 4,\n",
       " 'z': 210}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I define this variable since I will need it later for the model\n",
    "nltk_vocab_size = len(nltk_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_nltk(column, word_index):\n",
    "    \"\"\"\n",
    "    The function turns a text into padded arrays, ready as an input for the model\n",
    "    input:\n",
    "    - df: a pandas dataframe\n",
    "    - max_len: the length of the array for padding the comments\n",
    "    - encoding_support: needs to receive 'word_index'. The purpose of this argument is to allow using this function when creating\n",
    "      the datasets for the model.\n",
    "    The function return a matrix with the padded arrays\n",
    "    \"\"\"\n",
    "        \n",
    "    X_text = column.values\n",
    "    X_encode_list = []\n",
    "    max_len = 0 # Capture the max len of the sentences (equations)\n",
    "    for sent in X_text:\n",
    "        tokenized = wordpunct_tokenize(sent)\n",
    "        encoded = np.array([word_index[word] for word in tokenized if word in word_index])\n",
    "        X_encode_list.append(encoded)\n",
    "        length = len(encoded)\n",
    "        if length > max_len:\n",
    "            max_len = length\n",
    "    X_pad = pad_sequences(X_encode_list, maxlen=max_len, padding='post')\n",
    "        \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets = preprocess_with_nltk(data[\"lEquations_eos\"], nltk_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs = preprocess_with_nltk(data[\"lEquations_sos\"], nltk_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 57)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2, 105,   2,   3,  52,   7,   8,  72,  33,   7,   8,  13,\n",
       "        14,  15,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each **target** word receives a designated index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the word to index mapping for output language\n",
    "# word2idx_outputs = tokenizer_outputs.word_index\n",
    "# print('Found %s unique output tokens.' % len(word2idx_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examining word_index. This tokenuzer relates to both 'target' and 'target_inputs' (that's why it includes both <eos> and <sos>\n",
    "# word2idx_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(nltk_word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the size of the one-hot matrix (times the input length 57) as the output of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_len_target may be larger than this one if the data is really small, so I may consider enlarge it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (831, 33)\n",
      "encoder_inputs[0]: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "  12  63  23   6   3 119  47  39   2  36  14  25   1  10  12]\n",
      "\n",
      "decoder_inputs[0]: [14 16  1  2  3  4  5 17  7  8  9 10  2 11  4  5 12  7  8 13  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "decoder_inputs.shape: (831, 57)\n",
      "\n",
      "decoder_targets[0]: [ 1  2  3  4  5 17  7  8  9 10  2 11  4  5 12  7  8 13 14 15 16  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "decoder_targets.shape: (831, 58)\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[1])\n",
    "print()\n",
    "# I commented the following line due to use of NLTK\n",
    "#decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[1])\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "print()\n",
    "# I commented the following line due to use of NLTK\n",
    "#decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_targets[0]:\", decoder_targets[1])\n",
    "print(\"decoder_targets.shape:\", decoder_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am taking out the 19th element in the decoder input, '15' to make the shape the same\n",
    "decoder_targets = decoder_targets[: , np.r_[0:19, 20:58]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in decoder_targets)\n",
    "max_len_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # store all the pre-trained word vectors\n",
    "# print('Loading word vectors...')\n",
    "# word2vec = {}\n",
    "# with open(os.path.join('../large_files/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "#   # is just a space-separated text file in the format:\n",
    "#   # word vec[0] vec[1] vec[2] ...\n",
    "#   for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     vec = np.asarray(values[1:], dtype='float32')\n",
    "#     word2vec[word] = vec\n",
    "# print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # prepare embedding matrix\n",
    "# print('Filling pre-trained embeddings...')\n",
    "# num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "# embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "# for word, i in word2idx_inputs.items():\n",
    "#   if i < MAX_NUM_WORDS:\n",
    "#     embedding_vector = word2vec.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#       # words not found in embedding index will be all zeros.\n",
    "#       embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is instead of the row in the embedding matrix. If I use pre-trained I don't need this line\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now I am not using a pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  #weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    "  # trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create targets, since we cannot use sparse\n",
    "# categorical cross entropy when we have sequences\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output #########################################\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3, ...,   0,   0,   0],\n",
       "       [  1,   2,   3, ...,   0,   0,   0],\n",
       "       [  1,   2,  11, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [  1, 245,   5, ...,  16,   0,   0],\n",
       "       [  1,  23,   7, ...,   0,   0,   0],\n",
       "       [  1,   2,   3, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remind myself what decoder_targets is\n",
    "decoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign the values. So the one hot has three dimensions: the first is the 10000 data rows, the second is the 7 words of the \n",
    "# target, and the third is the 'num_word_output' which is the number of unique words in the target. So the 'word' varaible is the\n",
    "# value in the 'decoder_targets', for example 147, so this will be the index in the third dimension of the one_hot\n",
    "for i, d in enumerate(decoder_targets):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### build the model #####\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "\n",
    "embed_encod = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_state=True, # Because I use return_state=True I can have h and c, otherwise I would get only the encoder outputs\n",
    "  dropout=0.3 # dropout not available on gpu\n",
    ")\n",
    "encoder_outputs, h, c = encoder(embed_encod)\n",
    "# encoder_outputs, h = encoder(embed_encod) #gru\n",
    "\n",
    "# keep only the states to pass into decoder\n",
    "encoder_states = [h, c]\n",
    "# encoder_states = [state_h] # gru\n",
    "\n",
    "# Set up the decoder, using [h, c] as initial state.\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,)) # Important. I am using the max_len of the target(!) here\n",
    "\n",
    "# this word embedding will not use pre-trained vectors\n",
    "# although you could\n",
    "decoder_embedding = Embedding(num_words_output, LATENT_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "# since the decoder is a \"to-many\" model we want to have\n",
    "# return_sequences=True\n",
    "decoder_lstm = LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True,\n",
    "  return_state=True,\n",
    "  dropout=0.3 # dropout not available on gpu\n",
    ")\n",
    "# Below - like the encoder receives an input of 'embed_encode', here are two inputs: 1. the target text, represented by \n",
    "# decoder_input_x, 2. the the encoder_state (h,c) from the encoder\n",
    "# Also, I don't need the h and c as outputs (as I willl need them in the prediction)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "\n",
    "# decoder_outputs, _ = decoder_gru(\n",
    "#   decoder_inputs_x,\n",
    "#   initial_state=encoder_states\n",
    "# )\n",
    "\n",
    "# final dense layer for predictions\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model object\n",
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 57)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 33, 100)      54300       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 57, 256)      70912       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 365568      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 57, 256), (N 525312      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 57, 277)      71189       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,087,281\n",
      "Trainable params: 1,087,281\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 664 samples, validate on 167 samples\n",
      "Epoch 1/100\n",
      "664/664 [==============================] - ETA: 15s - loss: 5.6308 - acc: 0.00 - ETA: 8s - loss: 5.5939 - acc: 0.2762 - ETA: 5s - loss: 5.5532 - acc: 0.380 - ETA: 4s - loss: 5.5078 - acc: 0.426 - ETA: 3s - loss: 5.4487 - acc: 0.449 - ETA: 2s - loss: 5.3559 - acc: 0.462 - ETA: 1s - loss: 5.1963 - acc: 0.473 - ETA: 1s - loss: 4.9516 - acc: 0.484 - ETA: 0s - loss: 4.6769 - acc: 0.495 - ETA: 0s - loss: 4.5057 - acc: 0.500 - 5s 8ms/step - loss: 4.4626 - acc: 0.5009 - val_loss: 3.0143 - val_acc: 0.5367\n",
      "Epoch 2/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 2.9166 - acc: 0.550 - ETA: 2s - loss: 2.8751 - acc: 0.546 - ETA: 2s - loss: 2.7635 - acc: 0.553 - ETA: 2s - loss: 2.7366 - acc: 0.547 - ETA: 1s - loss: 2.6890 - acc: 0.546 - ETA: 1s - loss: 2.6523 - acc: 0.546 - ETA: 1s - loss: 2.6071 - acc: 0.549 - ETA: 0s - loss: 2.5771 - acc: 0.550 - ETA: 0s - loss: 2.5585 - acc: 0.548 - ETA: 0s - loss: 2.5367 - acc: 0.548 - 4s 6ms/step - loss: 2.5274 - acc: 0.5485 - val_loss: 2.2873 - val_acc: 0.5367\n",
      "Epoch 3/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 2.2158 - acc: 0.556 - ETA: 3s - loss: 2.1721 - acc: 0.561 - ETA: 2s - loss: 2.1949 - acc: 0.546 - ETA: 2s - loss: 2.1761 - acc: 0.542 - ETA: 2s - loss: 2.1521 - acc: 0.543 - ETA: 1s - loss: 2.1546 - acc: 0.539 - ETA: 1s - loss: 2.1485 - acc: 0.536 - ETA: 0s - loss: 2.1292 - acc: 0.538 - ETA: 0s - loss: 2.1085 - acc: 0.539 - ETA: 0s - loss: 2.0977 - acc: 0.539 - 4s 7ms/step - loss: 2.0960 - acc: 0.5385 - val_loss: 1.9833 - val_acc: 0.5235\n",
      "Epoch 4/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.9440 - acc: 0.522 - ETA: 3s - loss: 1.8720 - acc: 0.541 - ETA: 2s - loss: 1.8456 - acc: 0.544 - ETA: 2s - loss: 1.8352 - acc: 0.545 - ETA: 2s - loss: 1.8070 - acc: 0.548 - ETA: 1s - loss: 1.8106 - acc: 0.543 - ETA: 1s - loss: 1.8120 - acc: 0.540 - ETA: 0s - loss: 1.7897 - acc: 0.545 - ETA: 0s - loss: 1.7725 - acc: 0.550 - ETA: 0s - loss: 1.7554 - acc: 0.553 - 4s 7ms/step - loss: 1.7496 - acc: 0.5556 - val_loss: 1.6288 - val_acc: 0.5962\n",
      "Epoch 5/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.5723 - acc: 0.603 - ETA: 3s - loss: 1.5509 - acc: 0.608 - ETA: 2s - loss: 1.5648 - acc: 0.600 - ETA: 2s - loss: 1.5289 - acc: 0.611 - ETA: 2s - loss: 1.5240 - acc: 0.612 - ETA: 1s - loss: 1.5319 - acc: 0.609 - ETA: 1s - loss: 1.5286 - acc: 0.612 - ETA: 0s - loss: 1.5265 - acc: 0.614 - ETA: 0s - loss: 1.5231 - acc: 0.615 - ETA: 0s - loss: 1.5198 - acc: 0.615 - 4s 7ms/step - loss: 1.5175 - acc: 0.6164 - val_loss: 1.5219 - val_acc: 0.6241\n",
      "Epoch 6/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.4598 - acc: 0.632 - ETA: 3s - loss: 1.4993 - acc: 0.617 - ETA: 2s - loss: 1.5068 - acc: 0.615 - ETA: 2s - loss: 1.4834 - acc: 0.622 - ETA: 2s - loss: 1.4764 - acc: 0.624 - ETA: 1s - loss: 1.4684 - acc: 0.626 - ETA: 1s - loss: 1.4568 - acc: 0.631 - ETA: 0s - loss: 1.4410 - acc: 0.637 - ETA: 0s - loss: 1.4484 - acc: 0.636 - ETA: 0s - loss: 1.4369 - acc: 0.638 - 4s 7ms/step - loss: 1.4355 - acc: 0.6383 - val_loss: 1.4695 - val_acc: 0.6388\n",
      "Epoch 7/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.3932 - acc: 0.650 - ETA: 3s - loss: 1.3940 - acc: 0.653 - ETA: 2s - loss: 1.4002 - acc: 0.654 - ETA: 2s - loss: 1.4045 - acc: 0.655 - ETA: 2s - loss: 1.4201 - acc: 0.650 - ETA: 1s - loss: 1.4019 - acc: 0.655 - ETA: 1s - loss: 1.3918 - acc: 0.658 - ETA: 0s - loss: 1.3898 - acc: 0.659 - ETA: 0s - loss: 1.3854 - acc: 0.660 - ETA: 0s - loss: 1.3838 - acc: 0.660 - 5s 7ms/step - loss: 1.3838 - acc: 0.6605 - val_loss: 1.4280 - val_acc: 0.6625\n",
      "Epoch 8/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.3757 - acc: 0.669 - ETA: 3s - loss: 1.3918 - acc: 0.663 - ETA: 2s - loss: 1.3655 - acc: 0.667 - ETA: 2s - loss: 1.3780 - acc: 0.664 - ETA: 2s - loss: 1.3685 - acc: 0.666 - ETA: 1s - loss: 1.3769 - acc: 0.664 - ETA: 1s - loss: 1.3574 - acc: 0.668 - ETA: 0s - loss: 1.3544 - acc: 0.668 - ETA: 0s - loss: 1.3499 - acc: 0.669 - ETA: 0s - loss: 1.3435 - acc: 0.670 - 5s 7ms/step - loss: 1.3415 - acc: 0.6706 - val_loss: 1.3957 - val_acc: 0.6675\n",
      "Epoch 9/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.3022 - acc: 0.682 - ETA: 3s - loss: 1.3408 - acc: 0.676 - ETA: 3s - loss: 1.3110 - acc: 0.684 - ETA: 2s - loss: 1.3070 - acc: 0.686 - ETA: 2s - loss: 1.3298 - acc: 0.681 - ETA: 1s - loss: 1.3301 - acc: 0.681 - ETA: 1s - loss: 1.3253 - acc: 0.683 - ETA: 0s - loss: 1.3142 - acc: 0.684 - ETA: 0s - loss: 1.3098 - acc: 0.685 - ETA: 0s - loss: 1.3008 - acc: 0.688 - 5s 7ms/step - loss: 1.3005 - acc: 0.6887 - val_loss: 1.3735 - val_acc: 0.6832\n",
      "Epoch 10/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.3133 - acc: 0.684 - ETA: 3s - loss: 1.2945 - acc: 0.686 - ETA: 2s - loss: 1.3051 - acc: 0.684 - ETA: 2s - loss: 1.2900 - acc: 0.686 - ETA: 2s - loss: 1.2883 - acc: 0.687 - ETA: 1s - loss: 1.2769 - acc: 0.690 - ETA: 1s - loss: 1.2744 - acc: 0.691 - ETA: 0s - loss: 1.2639 - acc: 0.693 - ETA: 0s - loss: 1.2721 - acc: 0.691 - ETA: 0s - loss: 1.2656 - acc: 0.693 - 5s 7ms/step - loss: 1.2671 - acc: 0.6935 - val_loss: 1.3290 - val_acc: 0.6880\n",
      "Epoch 11/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.2607 - acc: 0.698 - ETA: 3s - loss: 1.2290 - acc: 0.703 - ETA: 2s - loss: 1.2413 - acc: 0.698 - ETA: 2s - loss: 1.2226 - acc: 0.702 - ETA: 2s - loss: 1.2167 - acc: 0.705 - ETA: 1s - loss: 1.2177 - acc: 0.706 - ETA: 1s - loss: 1.2191 - acc: 0.706 - ETA: 0s - loss: 1.2248 - acc: 0.706 - ETA: 0s - loss: 1.2281 - acc: 0.705 - ETA: 0s - loss: 1.2235 - acc: 0.707 - 5s 7ms/step - loss: 1.2251 - acc: 0.7070 - val_loss: 1.2920 - val_acc: 0.6981\n",
      "Epoch 12/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.2358 - acc: 0.702 - ETA: 3s - loss: 1.2029 - acc: 0.710 - ETA: 3s - loss: 1.1802 - acc: 0.714 - ETA: 2s - loss: 1.1727 - acc: 0.715 - ETA: 2s - loss: 1.1950 - acc: 0.710 - ETA: 1s - loss: 1.2030 - acc: 0.706 - ETA: 1s - loss: 1.1910 - acc: 0.709 - ETA: 0s - loss: 1.1848 - acc: 0.710 - ETA: 0s - loss: 1.1851 - acc: 0.711 - ETA: 0s - loss: 1.1849 - acc: 0.712 - 5s 7ms/step - loss: 1.1830 - acc: 0.7128 - val_loss: 1.2565 - val_acc: 0.7062\n",
      "Epoch 13/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.1288 - acc: 0.723 - ETA: 3s - loss: 1.1144 - acc: 0.728 - ETA: 3s - loss: 1.1470 - acc: 0.719 - ETA: 2s - loss: 1.1277 - acc: 0.721 - ETA: 2s - loss: 1.1212 - acc: 0.726 - ETA: 1s - loss: 1.1239 - acc: 0.727 - ETA: 1s - loss: 1.1292 - acc: 0.726 - ETA: 1s - loss: 1.1427 - acc: 0.723 - ETA: 0s - loss: 1.1399 - acc: 0.724 - ETA: 0s - loss: 1.1347 - acc: 0.726 - 5s 7ms/step - loss: 1.1336 - acc: 0.7261 - val_loss: 1.2015 - val_acc: 0.7131\n",
      "Epoch 14/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 1.0826 - acc: 0.727 - ETA: 3s - loss: 1.0602 - acc: 0.738 - ETA: 3s - loss: 1.0777 - acc: 0.737 - ETA: 2s - loss: 1.0815 - acc: 0.736 - ETA: 2s - loss: 1.0874 - acc: 0.734 - ETA: 1s - loss: 1.0827 - acc: 0.736 - ETA: 1s - loss: 1.0774 - acc: 0.737 - ETA: 0s - loss: 1.0704 - acc: 0.738 - ETA: 0s - loss: 1.0651 - acc: 0.739 - ETA: 0s - loss: 1.0643 - acc: 0.739 - 5s 7ms/step - loss: 1.0674 - acc: 0.7389 - val_loss: 1.1354 - val_acc: 0.7269\n",
      "Epoch 15/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 1.0715 - acc: 0.728 - ETA: 3s - loss: 1.0047 - acc: 0.750 - ETA: 3s - loss: 1.0117 - acc: 0.747 - ETA: 2s - loss: 1.0173 - acc: 0.748 - ETA: 2s - loss: 0.9986 - acc: 0.754 - ETA: 1s - loss: 0.9992 - acc: 0.753 - ETA: 1s - loss: 1.0054 - acc: 0.751 - ETA: 0s - loss: 1.0050 - acc: 0.751 - ETA: 0s - loss: 1.0070 - acc: 0.750 - ETA: 0s - loss: 1.0015 - acc: 0.751 - 5s 7ms/step - loss: 1.0014 - acc: 0.7512 - val_loss: 1.0850 - val_acc: 0.7437\n",
      "Epoch 16/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.9454 - acc: 0.768 - ETA: 3s - loss: 0.9409 - acc: 0.767 - ETA: 3s - loss: 0.9537 - acc: 0.762 - ETA: 2s - loss: 0.9674 - acc: 0.759 - ETA: 2s - loss: 0.9654 - acc: 0.757 - ETA: 1s - loss: 0.9568 - acc: 0.760 - ETA: 1s - loss: 0.9559 - acc: 0.760 - ETA: 1s - loss: 0.9429 - acc: 0.763 - ETA: 0s - loss: 0.9387 - acc: 0.764 - ETA: 0s - loss: 0.9444 - acc: 0.761 - 5s 7ms/step - loss: 0.9441 - acc: 0.7615 - val_loss: 1.0392 - val_acc: 0.7506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.9370 - acc: 0.769 - ETA: 3s - loss: 0.9297 - acc: 0.767 - ETA: 3s - loss: 0.9153 - acc: 0.769 - ETA: 2s - loss: 0.9073 - acc: 0.770 - ETA: 2s - loss: 0.9074 - acc: 0.771 - ETA: 1s - loss: 0.9042 - acc: 0.771 - ETA: 1s - loss: 0.9060 - acc: 0.770 - ETA: 1s - loss: 0.9028 - acc: 0.770 - ETA: 0s - loss: 0.9010 - acc: 0.771 - ETA: 0s - loss: 0.9004 - acc: 0.771 - 5s 7ms/step - loss: 0.8981 - acc: 0.7726 - val_loss: 1.0121 - val_acc: 0.7530\n",
      "Epoch 18/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.8394 - acc: 0.785 - ETA: 3s - loss: 0.8464 - acc: 0.788 - ETA: 2s - loss: 0.8645 - acc: 0.783 - ETA: 2s - loss: 0.8557 - acc: 0.784 - ETA: 2s - loss: 0.8640 - acc: 0.783 - ETA: 1s - loss: 0.8597 - acc: 0.784 - ETA: 1s - loss: 0.8544 - acc: 0.785 - ETA: 1s - loss: 0.8585 - acc: 0.784 - ETA: 0s - loss: 0.8606 - acc: 0.783 - ETA: 0s - loss: 0.8549 - acc: 0.784 - 5s 7ms/step - loss: 0.8555 - acc: 0.7846 - val_loss: 0.9734 - val_acc: 0.7615\n",
      "Epoch 19/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.8174 - acc: 0.782 - ETA: 3s - loss: 0.8038 - acc: 0.791 - ETA: 3s - loss: 0.8088 - acc: 0.794 - ETA: 2s - loss: 0.8109 - acc: 0.794 - ETA: 2s - loss: 0.8140 - acc: 0.794 - ETA: 1s - loss: 0.8124 - acc: 0.794 - ETA: 1s - loss: 0.8113 - acc: 0.794 - ETA: 0s - loss: 0.8116 - acc: 0.795 - ETA: 0s - loss: 0.8165 - acc: 0.793 - ETA: 0s - loss: 0.8154 - acc: 0.794 - 5s 7ms/step - loss: 0.8126 - acc: 0.7944 - val_loss: 0.9274 - val_acc: 0.7784\n",
      "Epoch 20/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.7564 - acc: 0.817 - ETA: 3s - loss: 0.7779 - acc: 0.806 - ETA: 2s - loss: 0.7758 - acc: 0.806 - ETA: 2s - loss: 0.7757 - acc: 0.806 - ETA: 2s - loss: 0.7795 - acc: 0.806 - ETA: 1s - loss: 0.7757 - acc: 0.807 - ETA: 1s - loss: 0.7787 - acc: 0.807 - ETA: 0s - loss: 0.7762 - acc: 0.807 - ETA: 0s - loss: 0.7760 - acc: 0.808 - ETA: 0s - loss: 0.7744 - acc: 0.809 - 5s 7ms/step - loss: 0.7693 - acc: 0.8112 - val_loss: 0.8831 - val_acc: 0.7946\n",
      "Epoch 21/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.7554 - acc: 0.807 - ETA: 3s - loss: 0.7241 - acc: 0.819 - ETA: 3s - loss: 0.7359 - acc: 0.817 - ETA: 2s - loss: 0.7322 - acc: 0.820 - ETA: 2s - loss: 0.7229 - acc: 0.824 - ETA: 1s - loss: 0.7232 - acc: 0.824 - ETA: 1s - loss: 0.7208 - acc: 0.824 - ETA: 0s - loss: 0.7228 - acc: 0.824 - ETA: 0s - loss: 0.7217 - acc: 0.824 - ETA: 0s - loss: 0.7289 - acc: 0.823 - 5s 7ms/step - loss: 0.7308 - acc: 0.8224 - val_loss: 0.8543 - val_acc: 0.8018\n",
      "Epoch 22/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.7031 - acc: 0.826 - ETA: 3s - loss: 0.7040 - acc: 0.830 - ETA: 3s - loss: 0.7217 - acc: 0.823 - ETA: 2s - loss: 0.7053 - acc: 0.826 - ETA: 2s - loss: 0.6901 - acc: 0.829 - ETA: 1s - loss: 0.7013 - acc: 0.826 - ETA: 1s - loss: 0.7014 - acc: 0.826 - ETA: 1s - loss: 0.7008 - acc: 0.827 - ETA: 0s - loss: 0.7024 - acc: 0.827 - ETA: 0s - loss: 0.6986 - acc: 0.828 - 5s 7ms/step - loss: 0.6969 - acc: 0.8289 - val_loss: 0.8177 - val_acc: 0.8091\n",
      "Epoch 23/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.6627 - acc: 0.830 - ETA: 3s - loss: 0.6700 - acc: 0.829 - ETA: 2s - loss: 0.6752 - acc: 0.828 - ETA: 2s - loss: 0.6780 - acc: 0.829 - ETA: 2s - loss: 0.6748 - acc: 0.830 - ETA: 1s - loss: 0.6774 - acc: 0.831 - ETA: 1s - loss: 0.6723 - acc: 0.833 - ETA: 0s - loss: 0.6707 - acc: 0.834 - ETA: 0s - loss: 0.6676 - acc: 0.835 - ETA: 0s - loss: 0.6672 - acc: 0.835 - 5s 7ms/step - loss: 0.6643 - acc: 0.8363 - val_loss: 0.7909 - val_acc: 0.8152\n",
      "Epoch 24/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.6489 - acc: 0.839 - ETA: 3s - loss: 0.6245 - acc: 0.842 - ETA: 3s - loss: 0.6142 - acc: 0.848 - ETA: 2s - loss: 0.6220 - acc: 0.847 - ETA: 2s - loss: 0.6262 - acc: 0.846 - ETA: 1s - loss: 0.6257 - acc: 0.846 - ETA: 1s - loss: 0.6268 - acc: 0.845 - ETA: 0s - loss: 0.6288 - acc: 0.844 - ETA: 0s - loss: 0.6293 - acc: 0.844 - ETA: 0s - loss: 0.6351 - acc: 0.843 - 5s 7ms/step - loss: 0.6364 - acc: 0.8431 - val_loss: 0.7701 - val_acc: 0.8195\n",
      "Epoch 25/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.5624 - acc: 0.862 - ETA: 3s - loss: 0.5983 - acc: 0.852 - ETA: 2s - loss: 0.6048 - acc: 0.850 - ETA: 2s - loss: 0.6168 - acc: 0.846 - ETA: 2s - loss: 0.6203 - acc: 0.845 - ETA: 1s - loss: 0.6320 - acc: 0.843 - ETA: 1s - loss: 0.6233 - acc: 0.845 - ETA: 0s - loss: 0.6138 - acc: 0.848 - ETA: 0s - loss: 0.6150 - acc: 0.847 - ETA: 0s - loss: 0.6147 - acc: 0.847 - 5s 7ms/step - loss: 0.6127 - acc: 0.8479 - val_loss: 0.7529 - val_acc: 0.8239\n",
      "Epoch 26/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.6058 - acc: 0.848 - ETA: 3s - loss: 0.5974 - acc: 0.849 - ETA: 3s - loss: 0.5965 - acc: 0.851 - ETA: 2s - loss: 0.5961 - acc: 0.852 - ETA: 2s - loss: 0.5897 - acc: 0.853 - ETA: 1s - loss: 0.5877 - acc: 0.854 - ETA: 1s - loss: 0.5896 - acc: 0.852 - ETA: 0s - loss: 0.5885 - acc: 0.853 - ETA: 0s - loss: 0.5878 - acc: 0.854 - ETA: 0s - loss: 0.5935 - acc: 0.852 - 5s 7ms/step - loss: 0.5915 - acc: 0.8531 - val_loss: 0.7369 - val_acc: 0.8262\n",
      "Epoch 27/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.5737 - acc: 0.860 - ETA: 3s - loss: 0.5888 - acc: 0.854 - ETA: 2s - loss: 0.5816 - acc: 0.856 - ETA: 2s - loss: 0.5746 - acc: 0.857 - ETA: 2s - loss: 0.5713 - acc: 0.859 - ETA: 1s - loss: 0.5670 - acc: 0.860 - ETA: 1s - loss: 0.5676 - acc: 0.860 - ETA: 0s - loss: 0.5668 - acc: 0.859 - ETA: 0s - loss: 0.5710 - acc: 0.858 - ETA: 0s - loss: 0.5717 - acc: 0.858 - 5s 7ms/step - loss: 0.5723 - acc: 0.8575 - val_loss: 0.7206 - val_acc: 0.8294\n",
      "Epoch 28/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.5754 - acc: 0.852 - ETA: 3s - loss: 0.5399 - acc: 0.862 - ETA: 3s - loss: 0.5540 - acc: 0.859 - ETA: 2s - loss: 0.5588 - acc: 0.859 - ETA: 2s - loss: 0.5598 - acc: 0.858 - ETA: 1s - loss: 0.5658 - acc: 0.857 - ETA: 1s - loss: 0.5648 - acc: 0.857 - ETA: 1s - loss: 0.5597 - acc: 0.859 - ETA: 0s - loss: 0.5563 - acc: 0.860 - ETA: 0s - loss: 0.5550 - acc: 0.860 - 5s 7ms/step - loss: 0.5553 - acc: 0.8603 - val_loss: 0.7150 - val_acc: 0.8308\n",
      "Epoch 29/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4931 - acc: 0.876 - ETA: 3s - loss: 0.5302 - acc: 0.868 - ETA: 3s - loss: 0.5372 - acc: 0.865 - ETA: 2s - loss: 0.5350 - acc: 0.866 - ETA: 2s - loss: 0.5389 - acc: 0.865 - ETA: 1s - loss: 0.5338 - acc: 0.865 - ETA: 1s - loss: 0.5347 - acc: 0.865 - ETA: 1s - loss: 0.5379 - acc: 0.864 - ETA: 0s - loss: 0.5400 - acc: 0.864 - ETA: 0s - loss: 0.5411 - acc: 0.864 - 5s 8ms/step - loss: 0.5406 - acc: 0.8642 - val_loss: 0.6915 - val_acc: 0.8348\n",
      "Epoch 30/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.5172 - acc: 0.867 - ETA: 3s - loss: 0.5346 - acc: 0.862 - ETA: 3s - loss: 0.5261 - acc: 0.865 - ETA: 2s - loss: 0.5353 - acc: 0.864 - ETA: 2s - loss: 0.5344 - acc: 0.863 - ETA: 1s - loss: 0.5259 - acc: 0.866 - ETA: 1s - loss: 0.5253 - acc: 0.866 - ETA: 1s - loss: 0.5217 - acc: 0.868 - ETA: 0s - loss: 0.5223 - acc: 0.868 - ETA: 0s - loss: 0.5263 - acc: 0.867 - 5s 7ms/step - loss: 0.5265 - acc: 0.8668 - val_loss: 0.6839 - val_acc: 0.8384\n",
      "Epoch 31/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.5492 - acc: 0.859 - ETA: 3s - loss: 0.5015 - acc: 0.874 - ETA: 3s - loss: 0.4965 - acc: 0.874 - ETA: 2s - loss: 0.5056 - acc: 0.872 - ETA: 2s - loss: 0.5074 - acc: 0.871 - ETA: 1s - loss: 0.5097 - acc: 0.870 - ETA: 1s - loss: 0.5059 - acc: 0.871 - ETA: 1s - loss: 0.5146 - acc: 0.869 - ETA: 0s - loss: 0.5104 - acc: 0.870 - ETA: 0s - loss: 0.5118 - acc: 0.869 - 5s 7ms/step - loss: 0.5149 - acc: 0.8692 - val_loss: 0.6708 - val_acc: 0.8417\n",
      "Epoch 32/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.5082 - acc: 0.866 - ETA: 3s - loss: 0.5088 - acc: 0.867 - ETA: 3s - loss: 0.5394 - acc: 0.860 - ETA: 2s - loss: 0.5307 - acc: 0.864 - ETA: 2s - loss: 0.5142 - acc: 0.870 - ETA: 1s - loss: 0.5136 - acc: 0.869 - ETA: 1s - loss: 0.5195 - acc: 0.867 - ETA: 1s - loss: 0.5088 - acc: 0.870 - ETA: 0s - loss: 0.5032 - acc: 0.870 - ETA: 0s - loss: 0.4992 - acc: 0.872 - 5s 7ms/step - loss: 0.5013 - acc: 0.8719 - val_loss: 0.6599 - val_acc: 0.8426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.5081 - acc: 0.866 - ETA: 3s - loss: 0.4730 - acc: 0.875 - ETA: 3s - loss: 0.4659 - acc: 0.879 - ETA: 2s - loss: 0.4828 - acc: 0.874 - ETA: 2s - loss: 0.4863 - acc: 0.875 - ETA: 1s - loss: 0.4927 - acc: 0.873 - ETA: 1s - loss: 0.4946 - acc: 0.873 - ETA: 1s - loss: 0.4978 - acc: 0.872 - ETA: 0s - loss: 0.4969 - acc: 0.873 - ETA: 0s - loss: 0.4938 - acc: 0.874 - 5s 7ms/step - loss: 0.4946 - acc: 0.8738 - val_loss: 0.6528 - val_acc: 0.8459\n",
      "Epoch 34/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.5082 - acc: 0.875 - ETA: 3s - loss: 0.4730 - acc: 0.882 - ETA: 3s - loss: 0.4848 - acc: 0.876 - ETA: 2s - loss: 0.4825 - acc: 0.877 - ETA: 2s - loss: 0.4776 - acc: 0.878 - ETA: 1s - loss: 0.4799 - acc: 0.878 - ETA: 1s - loss: 0.4804 - acc: 0.878 - ETA: 1s - loss: 0.4857 - acc: 0.876 - ETA: 0s - loss: 0.4854 - acc: 0.876 - ETA: 0s - loss: 0.4815 - acc: 0.877 - 5s 7ms/step - loss: 0.4798 - acc: 0.8777 - val_loss: 0.6428 - val_acc: 0.8475\n",
      "Epoch 35/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.5019 - acc: 0.874 - ETA: 3s - loss: 0.4867 - acc: 0.874 - ETA: 3s - loss: 0.4954 - acc: 0.873 - ETA: 2s - loss: 0.4840 - acc: 0.875 - ETA: 2s - loss: 0.4771 - acc: 0.877 - ETA: 1s - loss: 0.4790 - acc: 0.877 - ETA: 1s - loss: 0.4668 - acc: 0.879 - ETA: 1s - loss: 0.4648 - acc: 0.880 - ETA: 0s - loss: 0.4692 - acc: 0.879 - ETA: 0s - loss: 0.4684 - acc: 0.880 - 5s 7ms/step - loss: 0.4689 - acc: 0.8801 - val_loss: 0.6462 - val_acc: 0.8465\n",
      "Epoch 36/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4814 - acc: 0.882 - ETA: 3s - loss: 0.4660 - acc: 0.882 - ETA: 3s - loss: 0.4516 - acc: 0.886 - ETA: 2s - loss: 0.4439 - acc: 0.887 - ETA: 2s - loss: 0.4498 - acc: 0.884 - ETA: 1s - loss: 0.4603 - acc: 0.881 - ETA: 1s - loss: 0.4623 - acc: 0.881 - ETA: 1s - loss: 0.4607 - acc: 0.881 - ETA: 0s - loss: 0.4661 - acc: 0.878 - ETA: 0s - loss: 0.4653 - acc: 0.879 - 5s 7ms/step - loss: 0.4640 - acc: 0.8800 - val_loss: 0.6374 - val_acc: 0.8446\n",
      "Epoch 37/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4368 - acc: 0.883 - ETA: 3s - loss: 0.4465 - acc: 0.880 - ETA: 3s - loss: 0.4494 - acc: 0.881 - ETA: 2s - loss: 0.4405 - acc: 0.884 - ETA: 2s - loss: 0.4413 - acc: 0.885 - ETA: 1s - loss: 0.4365 - acc: 0.887 - ETA: 1s - loss: 0.4374 - acc: 0.887 - ETA: 0s - loss: 0.4469 - acc: 0.884 - ETA: 0s - loss: 0.4506 - acc: 0.883 - ETA: 0s - loss: 0.4532 - acc: 0.882 - 5s 7ms/step - loss: 0.4547 - acc: 0.8820 - val_loss: 0.6274 - val_acc: 0.8486\n",
      "Epoch 38/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4640 - acc: 0.878 - ETA: 3s - loss: 0.4471 - acc: 0.882 - ETA: 3s - loss: 0.4596 - acc: 0.880 - ETA: 2s - loss: 0.4594 - acc: 0.881 - ETA: 2s - loss: 0.4491 - acc: 0.884 - ETA: 1s - loss: 0.4513 - acc: 0.883 - ETA: 1s - loss: 0.4520 - acc: 0.882 - ETA: 0s - loss: 0.4487 - acc: 0.884 - ETA: 0s - loss: 0.4472 - acc: 0.884 - ETA: 0s - loss: 0.4454 - acc: 0.884 - 5s 7ms/step - loss: 0.4460 - acc: 0.8842 - val_loss: 0.6172 - val_acc: 0.8502\n",
      "Epoch 39/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4548 - acc: 0.876 - ETA: 3s - loss: 0.4339 - acc: 0.885 - ETA: 3s - loss: 0.4275 - acc: 0.887 - ETA: 2s - loss: 0.4339 - acc: 0.887 - ETA: 2s - loss: 0.4394 - acc: 0.886 - ETA: 1s - loss: 0.4419 - acc: 0.885 - ETA: 1s - loss: 0.4431 - acc: 0.884 - ETA: 0s - loss: 0.4430 - acc: 0.884 - ETA: 0s - loss: 0.4425 - acc: 0.884 - ETA: 0s - loss: 0.4365 - acc: 0.885 - 5s 7ms/step - loss: 0.4369 - acc: 0.8858 - val_loss: 0.6096 - val_acc: 0.8535\n",
      "Epoch 40/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4798 - acc: 0.878 - ETA: 3s - loss: 0.4288 - acc: 0.890 - ETA: 3s - loss: 0.4284 - acc: 0.890 - ETA: 2s - loss: 0.4348 - acc: 0.887 - ETA: 2s - loss: 0.4351 - acc: 0.887 - ETA: 1s - loss: 0.4353 - acc: 0.886 - ETA: 1s - loss: 0.4412 - acc: 0.884 - ETA: 0s - loss: 0.4379 - acc: 0.885 - ETA: 0s - loss: 0.4343 - acc: 0.886 - ETA: 0s - loss: 0.4318 - acc: 0.887 - 5s 7ms/step - loss: 0.4298 - acc: 0.8881 - val_loss: 0.6082 - val_acc: 0.8536\n",
      "Epoch 41/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.4191 - acc: 0.886 - ETA: 3s - loss: 0.4002 - acc: 0.893 - ETA: 3s - loss: 0.4104 - acc: 0.892 - ETA: 2s - loss: 0.4168 - acc: 0.891 - ETA: 2s - loss: 0.4129 - acc: 0.891 - ETA: 1s - loss: 0.4221 - acc: 0.888 - ETA: 1s - loss: 0.4231 - acc: 0.888 - ETA: 1s - loss: 0.4269 - acc: 0.888 - ETA: 0s - loss: 0.4249 - acc: 0.888 - ETA: 0s - loss: 0.4226 - acc: 0.889 - 5s 7ms/step - loss: 0.4212 - acc: 0.8895 - val_loss: 0.6032 - val_acc: 0.8547\n",
      "Epoch 42/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4256 - acc: 0.891 - ETA: 3s - loss: 0.4321 - acc: 0.890 - ETA: 3s - loss: 0.4207 - acc: 0.891 - ETA: 2s - loss: 0.4186 - acc: 0.891 - ETA: 2s - loss: 0.4173 - acc: 0.891 - ETA: 1s - loss: 0.4163 - acc: 0.891 - ETA: 1s - loss: 0.4229 - acc: 0.889 - ETA: 1s - loss: 0.4200 - acc: 0.889 - ETA: 0s - loss: 0.4162 - acc: 0.890 - ETA: 0s - loss: 0.4134 - acc: 0.891 - 5s 7ms/step - loss: 0.4163 - acc: 0.8909 - val_loss: 0.5994 - val_acc: 0.8556\n",
      "Epoch 43/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4265 - acc: 0.884 - ETA: 3s - loss: 0.4169 - acc: 0.890 - ETA: 3s - loss: 0.4175 - acc: 0.889 - ETA: 2s - loss: 0.4090 - acc: 0.891 - ETA: 2s - loss: 0.4114 - acc: 0.889 - ETA: 1s - loss: 0.4125 - acc: 0.889 - ETA: 1s - loss: 0.4148 - acc: 0.889 - ETA: 0s - loss: 0.4125 - acc: 0.890 - ETA: 0s - loss: 0.4091 - acc: 0.891 - ETA: 0s - loss: 0.4110 - acc: 0.891 - 5s 7ms/step - loss: 0.4076 - acc: 0.8923 - val_loss: 0.5996 - val_acc: 0.8566\n",
      "Epoch 44/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3926 - acc: 0.897 - ETA: 3s - loss: 0.3981 - acc: 0.896 - ETA: 3s - loss: 0.3967 - acc: 0.896 - ETA: 2s - loss: 0.3991 - acc: 0.894 - ETA: 2s - loss: 0.3998 - acc: 0.894 - ETA: 1s - loss: 0.4041 - acc: 0.893 - ETA: 1s - loss: 0.4072 - acc: 0.892 - ETA: 0s - loss: 0.4058 - acc: 0.892 - ETA: 0s - loss: 0.4042 - acc: 0.893 - ETA: 0s - loss: 0.4021 - acc: 0.893 - 5s 7ms/step - loss: 0.4018 - acc: 0.8942 - val_loss: 0.5866 - val_acc: 0.8594\n",
      "Epoch 45/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3830 - acc: 0.899 - ETA: 3s - loss: 0.3980 - acc: 0.895 - ETA: 3s - loss: 0.3880 - acc: 0.898 - ETA: 2s - loss: 0.3953 - acc: 0.896 - ETA: 2s - loss: 0.3966 - acc: 0.895 - ETA: 1s - loss: 0.3952 - acc: 0.895 - ETA: 1s - loss: 0.3903 - acc: 0.896 - ETA: 1s - loss: 0.3858 - acc: 0.898 - ETA: 0s - loss: 0.3890 - acc: 0.897 - ETA: 0s - loss: 0.3887 - acc: 0.897 - 5s 7ms/step - loss: 0.3931 - acc: 0.8965 - val_loss: 0.5881 - val_acc: 0.8588\n",
      "Epoch 46/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.4014 - acc: 0.892 - ETA: 3s - loss: 0.3993 - acc: 0.893 - ETA: 3s - loss: 0.3910 - acc: 0.896 - ETA: 2s - loss: 0.3932 - acc: 0.895 - ETA: 2s - loss: 0.3859 - acc: 0.897 - ETA: 1s - loss: 0.3980 - acc: 0.894 - ETA: 1s - loss: 0.3972 - acc: 0.894 - ETA: 0s - loss: 0.3919 - acc: 0.895 - ETA: 0s - loss: 0.3898 - acc: 0.896 - ETA: 0s - loss: 0.3922 - acc: 0.896 - 5s 7ms/step - loss: 0.3905 - acc: 0.8967 - val_loss: 0.5899 - val_acc: 0.8593\n",
      "Epoch 47/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3844 - acc: 0.897 - ETA: 3s - loss: 0.3981 - acc: 0.895 - ETA: 3s - loss: 0.3945 - acc: 0.895 - ETA: 2s - loss: 0.3906 - acc: 0.895 - ETA: 2s - loss: 0.3801 - acc: 0.898 - ETA: 1s - loss: 0.3824 - acc: 0.899 - ETA: 1s - loss: 0.3839 - acc: 0.898 - ETA: 0s - loss: 0.3811 - acc: 0.899 - ETA: 0s - loss: 0.3827 - acc: 0.898 - ETA: 0s - loss: 0.3822 - acc: 0.898 - 5s 7ms/step - loss: 0.3829 - acc: 0.8980 - val_loss: 0.5814 - val_acc: 0.8596\n",
      "Epoch 48/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3644 - acc: 0.908 - ETA: 3s - loss: 0.3797 - acc: 0.903 - ETA: 2s - loss: 0.3828 - acc: 0.900 - ETA: 2s - loss: 0.3762 - acc: 0.901 - ETA: 2s - loss: 0.3746 - acc: 0.900 - ETA: 1s - loss: 0.3741 - acc: 0.900 - ETA: 1s - loss: 0.3785 - acc: 0.899 - ETA: 0s - loss: 0.3739 - acc: 0.901 - ETA: 0s - loss: 0.3750 - acc: 0.900 - ETA: 0s - loss: 0.3747 - acc: 0.900 - 5s 7ms/step - loss: 0.3758 - acc: 0.9000 - val_loss: 0.5863 - val_acc: 0.8609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3631 - acc: 0.899 - ETA: 3s - loss: 0.3635 - acc: 0.902 - ETA: 3s - loss: 0.3599 - acc: 0.904 - ETA: 2s - loss: 0.3694 - acc: 0.902 - ETA: 2s - loss: 0.3720 - acc: 0.901 - ETA: 1s - loss: 0.3682 - acc: 0.902 - ETA: 1s - loss: 0.3664 - acc: 0.903 - ETA: 0s - loss: 0.3667 - acc: 0.903 - ETA: 0s - loss: 0.3685 - acc: 0.902 - ETA: 0s - loss: 0.3690 - acc: 0.901 - 5s 7ms/step - loss: 0.3719 - acc: 0.9013 - val_loss: 0.5756 - val_acc: 0.8619\n",
      "Epoch 50/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3465 - acc: 0.909 - ETA: 3s - loss: 0.3558 - acc: 0.905 - ETA: 3s - loss: 0.3508 - acc: 0.906 - ETA: 2s - loss: 0.3607 - acc: 0.903 - ETA: 2s - loss: 0.3575 - acc: 0.904 - ETA: 1s - loss: 0.3602 - acc: 0.904 - ETA: 1s - loss: 0.3584 - acc: 0.904 - ETA: 0s - loss: 0.3628 - acc: 0.903 - ETA: 0s - loss: 0.3674 - acc: 0.902 - ETA: 0s - loss: 0.3667 - acc: 0.901 - 5s 7ms/step - loss: 0.3668 - acc: 0.9014 - val_loss: 0.5766 - val_acc: 0.8634\n",
      "Epoch 51/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3972 - acc: 0.887 - ETA: 3s - loss: 0.3772 - acc: 0.895 - ETA: 3s - loss: 0.3663 - acc: 0.899 - ETA: 2s - loss: 0.3586 - acc: 0.902 - ETA: 2s - loss: 0.3566 - acc: 0.902 - ETA: 1s - loss: 0.3574 - acc: 0.903 - ETA: 1s - loss: 0.3523 - acc: 0.906 - ETA: 0s - loss: 0.3538 - acc: 0.905 - ETA: 0s - loss: 0.3649 - acc: 0.902 - ETA: 0s - loss: 0.3629 - acc: 0.902 - 5s 7ms/step - loss: 0.3620 - acc: 0.9027 - val_loss: 0.5753 - val_acc: 0.8625\n",
      "Epoch 52/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3372 - acc: 0.908 - ETA: 3s - loss: 0.3308 - acc: 0.909 - ETA: 3s - loss: 0.3411 - acc: 0.907 - ETA: 2s - loss: 0.3416 - acc: 0.906 - ETA: 2s - loss: 0.3411 - acc: 0.907 - ETA: 1s - loss: 0.3646 - acc: 0.900 - ETA: 1s - loss: 0.3652 - acc: 0.900 - ETA: 1s - loss: 0.3621 - acc: 0.901 - ETA: 0s - loss: 0.3606 - acc: 0.902 - ETA: 0s - loss: 0.3589 - acc: 0.902 - 5s 7ms/step - loss: 0.3589 - acc: 0.9027 - val_loss: 0.5679 - val_acc: 0.8635\n",
      "Epoch 53/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3485 - acc: 0.904 - ETA: 3s - loss: 0.3388 - acc: 0.907 - ETA: 3s - loss: 0.3544 - acc: 0.903 - ETA: 2s - loss: 0.3623 - acc: 0.901 - ETA: 2s - loss: 0.3581 - acc: 0.902 - ETA: 1s - loss: 0.3581 - acc: 0.902 - ETA: 1s - loss: 0.3545 - acc: 0.903 - ETA: 1s - loss: 0.3527 - acc: 0.903 - ETA: 0s - loss: 0.3532 - acc: 0.903 - ETA: 0s - loss: 0.3538 - acc: 0.903 - 5s 7ms/step - loss: 0.3526 - acc: 0.9042 - val_loss: 0.5682 - val_acc: 0.8647\n",
      "Epoch 54/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3703 - acc: 0.900 - ETA: 3s - loss: 0.3407 - acc: 0.908 - ETA: 3s - loss: 0.3570 - acc: 0.904 - ETA: 2s - loss: 0.3458 - acc: 0.907 - ETA: 2s - loss: 0.3416 - acc: 0.909 - ETA: 1s - loss: 0.3459 - acc: 0.907 - ETA: 1s - loss: 0.3432 - acc: 0.908 - ETA: 1s - loss: 0.3482 - acc: 0.906 - ETA: 0s - loss: 0.3438 - acc: 0.907 - ETA: 0s - loss: 0.3470 - acc: 0.906 - 5s 7ms/step - loss: 0.3458 - acc: 0.9065 - val_loss: 0.5670 - val_acc: 0.8646\n",
      "Epoch 55/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3920 - acc: 0.894 - ETA: 3s - loss: 0.3543 - acc: 0.902 - ETA: 3s - loss: 0.3338 - acc: 0.908 - ETA: 2s - loss: 0.3416 - acc: 0.907 - ETA: 2s - loss: 0.3386 - acc: 0.908 - ETA: 1s - loss: 0.3363 - acc: 0.908 - ETA: 1s - loss: 0.3371 - acc: 0.908 - ETA: 1s - loss: 0.3382 - acc: 0.907 - ETA: 0s - loss: 0.3382 - acc: 0.907 - ETA: 0s - loss: 0.3395 - acc: 0.907 - 5s 7ms/step - loss: 0.3409 - acc: 0.9071 - val_loss: 0.5636 - val_acc: 0.8648\n",
      "Epoch 56/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3235 - acc: 0.905 - ETA: 3s - loss: 0.3144 - acc: 0.910 - ETA: 3s - loss: 0.3140 - acc: 0.913 - ETA: 2s - loss: 0.3303 - acc: 0.910 - ETA: 2s - loss: 0.3346 - acc: 0.908 - ETA: 1s - loss: 0.3317 - acc: 0.909 - ETA: 1s - loss: 0.3311 - acc: 0.909 - ETA: 1s - loss: 0.3361 - acc: 0.908 - ETA: 0s - loss: 0.3362 - acc: 0.908 - ETA: 0s - loss: 0.3371 - acc: 0.907 - 5s 7ms/step - loss: 0.3352 - acc: 0.9087 - val_loss: 0.5609 - val_acc: 0.8663\n",
      "Epoch 57/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3399 - acc: 0.907 - ETA: 3s - loss: 0.3321 - acc: 0.909 - ETA: 3s - loss: 0.3203 - acc: 0.912 - ETA: 2s - loss: 0.3154 - acc: 0.912 - ETA: 2s - loss: 0.3281 - acc: 0.909 - ETA: 1s - loss: 0.3235 - acc: 0.911 - ETA: 1s - loss: 0.3225 - acc: 0.911 - ETA: 1s - loss: 0.3215 - acc: 0.912 - ETA: 0s - loss: 0.3277 - acc: 0.910 - ETA: 0s - loss: 0.3283 - acc: 0.910 - 5s 7ms/step - loss: 0.3286 - acc: 0.9102 - val_loss: 0.5585 - val_acc: 0.8669\n",
      "Epoch 58/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3384 - acc: 0.905 - ETA: 3s - loss: 0.3314 - acc: 0.907 - ETA: 3s - loss: 0.3127 - acc: 0.913 - ETA: 2s - loss: 0.3102 - acc: 0.913 - ETA: 2s - loss: 0.3125 - acc: 0.913 - ETA: 1s - loss: 0.3207 - acc: 0.911 - ETA: 1s - loss: 0.3191 - acc: 0.912 - ETA: 1s - loss: 0.3200 - acc: 0.911 - ETA: 0s - loss: 0.3191 - acc: 0.912 - ETA: 0s - loss: 0.3213 - acc: 0.912 - 5s 7ms/step - loss: 0.3224 - acc: 0.9118 - val_loss: 0.5619 - val_acc: 0.8675\n",
      "Epoch 59/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3189 - acc: 0.909 - ETA: 3s - loss: 0.3160 - acc: 0.908 - ETA: 3s - loss: 0.3320 - acc: 0.907 - ETA: 2s - loss: 0.3302 - acc: 0.908 - ETA: 2s - loss: 0.3237 - acc: 0.910 - ETA: 2s - loss: 0.3271 - acc: 0.909 - ETA: 1s - loss: 0.3244 - acc: 0.910 - ETA: 1s - loss: 0.3225 - acc: 0.910 - ETA: 0s - loss: 0.3259 - acc: 0.909 - ETA: 0s - loss: 0.3229 - acc: 0.910 - 5s 8ms/step - loss: 0.3229 - acc: 0.9102 - val_loss: 0.5573 - val_acc: 0.8665\n",
      "Epoch 60/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3123 - acc: 0.908 - ETA: 3s - loss: 0.3097 - acc: 0.910 - ETA: 3s - loss: 0.3166 - acc: 0.909 - ETA: 2s - loss: 0.3133 - acc: 0.911 - ETA: 2s - loss: 0.3173 - acc: 0.910 - ETA: 1s - loss: 0.3167 - acc: 0.910 - ETA: 1s - loss: 0.3117 - acc: 0.912 - ETA: 1s - loss: 0.3165 - acc: 0.911 - ETA: 0s - loss: 0.3178 - acc: 0.911 - ETA: 0s - loss: 0.3208 - acc: 0.910 - 5s 8ms/step - loss: 0.3232 - acc: 0.9103 - val_loss: 0.5600 - val_acc: 0.8684\n",
      "Epoch 61/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3000 - acc: 0.917 - ETA: 3s - loss: 0.3054 - acc: 0.913 - ETA: 3s - loss: 0.3158 - acc: 0.910 - ETA: 2s - loss: 0.3130 - acc: 0.912 - ETA: 2s - loss: 0.3223 - acc: 0.910 - ETA: 1s - loss: 0.3176 - acc: 0.912 - ETA: 1s - loss: 0.3256 - acc: 0.910 - ETA: 1s - loss: 0.3247 - acc: 0.910 - ETA: 0s - loss: 0.3206 - acc: 0.910 - ETA: 0s - loss: 0.3222 - acc: 0.910 - 5s 8ms/step - loss: 0.3213 - acc: 0.9107 - val_loss: 0.5608 - val_acc: 0.8666\n",
      "Epoch 62/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3374 - acc: 0.906 - ETA: 3s - loss: 0.3217 - acc: 0.908 - ETA: 3s - loss: 0.3227 - acc: 0.908 - ETA: 2s - loss: 0.3177 - acc: 0.911 - ETA: 2s - loss: 0.3138 - acc: 0.912 - ETA: 1s - loss: 0.3101 - acc: 0.914 - ETA: 1s - loss: 0.3087 - acc: 0.914 - ETA: 1s - loss: 0.3127 - acc: 0.913 - ETA: 0s - loss: 0.3116 - acc: 0.913 - ETA: 0s - loss: 0.3115 - acc: 0.913 - 5s 8ms/step - loss: 0.3135 - acc: 0.9127 - val_loss: 0.5560 - val_acc: 0.8697\n",
      "Epoch 63/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3244 - acc: 0.909 - ETA: 3s - loss: 0.3150 - acc: 0.911 - ETA: 3s - loss: 0.3094 - acc: 0.913 - ETA: 2s - loss: 0.3125 - acc: 0.911 - ETA: 2s - loss: 0.3084 - acc: 0.912 - ETA: 1s - loss: 0.3077 - acc: 0.913 - ETA: 1s - loss: 0.3081 - acc: 0.913 - ETA: 1s - loss: 0.3033 - acc: 0.914 - ETA: 0s - loss: 0.3019 - acc: 0.915 - ETA: 0s - loss: 0.3048 - acc: 0.914 - 5s 8ms/step - loss: 0.3047 - acc: 0.9147 - val_loss: 0.5525 - val_acc: 0.8679\n",
      "Epoch 64/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3033 - acc: 0.916 - ETA: 3s - loss: 0.3025 - acc: 0.916 - ETA: 3s - loss: 0.3184 - acc: 0.912 - ETA: 2s - loss: 0.3057 - acc: 0.915 - ETA: 2s - loss: 0.3014 - acc: 0.916 - ETA: 1s - loss: 0.2993 - acc: 0.917 - ETA: 1s - loss: 0.3032 - acc: 0.916 - ETA: 1s - loss: 0.3031 - acc: 0.915 - ETA: 0s - loss: 0.3046 - acc: 0.915 - ETA: 0s - loss: 0.3024 - acc: 0.916 - 5s 7ms/step - loss: 0.3037 - acc: 0.9155 - val_loss: 0.5511 - val_acc: 0.8693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3240 - acc: 0.908 - ETA: 3s - loss: 0.3174 - acc: 0.910 - ETA: 3s - loss: 0.3138 - acc: 0.912 - ETA: 2s - loss: 0.3087 - acc: 0.913 - ETA: 2s - loss: 0.3045 - acc: 0.913 - ETA: 1s - loss: 0.3038 - acc: 0.914 - ETA: 1s - loss: 0.3022 - acc: 0.915 - ETA: 1s - loss: 0.3019 - acc: 0.915 - ETA: 0s - loss: 0.3014 - acc: 0.915 - ETA: 0s - loss: 0.2995 - acc: 0.916 - 5s 8ms/step - loss: 0.2982 - acc: 0.9166 - val_loss: 0.5621 - val_acc: 0.8694\n",
      "Epoch 66/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.3157 - acc: 0.915 - ETA: 3s - loss: 0.2970 - acc: 0.917 - ETA: 3s - loss: 0.2922 - acc: 0.919 - ETA: 2s - loss: 0.2981 - acc: 0.918 - ETA: 2s - loss: 0.3002 - acc: 0.917 - ETA: 1s - loss: 0.3019 - acc: 0.916 - ETA: 1s - loss: 0.3035 - acc: 0.915 - ETA: 1s - loss: 0.2992 - acc: 0.917 - ETA: 0s - loss: 0.2982 - acc: 0.917 - ETA: 0s - loss: 0.2960 - acc: 0.918 - 5s 8ms/step - loss: 0.2943 - acc: 0.9182 - val_loss: 0.5521 - val_acc: 0.8707\n",
      "Epoch 67/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3029 - acc: 0.913 - ETA: 3s - loss: 0.2865 - acc: 0.918 - ETA: 3s - loss: 0.2814 - acc: 0.920 - ETA: 2s - loss: 0.2950 - acc: 0.915 - ETA: 2s - loss: 0.2940 - acc: 0.916 - ETA: 1s - loss: 0.2948 - acc: 0.916 - ETA: 1s - loss: 0.2958 - acc: 0.916 - ETA: 1s - loss: 0.2984 - acc: 0.916 - ETA: 0s - loss: 0.2972 - acc: 0.917 - ETA: 0s - loss: 0.2919 - acc: 0.918 - 5s 8ms/step - loss: 0.2907 - acc: 0.9189 - val_loss: 0.5709 - val_acc: 0.8666\n",
      "Epoch 68/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2897 - acc: 0.921 - ETA: 3s - loss: 0.2910 - acc: 0.918 - ETA: 3s - loss: 0.2907 - acc: 0.918 - ETA: 2s - loss: 0.3035 - acc: 0.914 - ETA: 2s - loss: 0.2992 - acc: 0.915 - ETA: 1s - loss: 0.2924 - acc: 0.918 - ETA: 1s - loss: 0.2919 - acc: 0.917 - ETA: 1s - loss: 0.2960 - acc: 0.916 - ETA: 0s - loss: 0.2903 - acc: 0.918 - ETA: 0s - loss: 0.2914 - acc: 0.918 - 5s 8ms/step - loss: 0.2912 - acc: 0.9184 - val_loss: 0.5549 - val_acc: 0.8686\n",
      "Epoch 69/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.3067 - acc: 0.915 - ETA: 3s - loss: 0.2853 - acc: 0.920 - ETA: 3s - loss: 0.2745 - acc: 0.925 - ETA: 2s - loss: 0.2790 - acc: 0.923 - ETA: 2s - loss: 0.2849 - acc: 0.920 - ETA: 1s - loss: 0.2823 - acc: 0.920 - ETA: 1s - loss: 0.2832 - acc: 0.920 - ETA: 1s - loss: 0.2835 - acc: 0.919 - ETA: 0s - loss: 0.2835 - acc: 0.919 - ETA: 0s - loss: 0.2872 - acc: 0.919 - 5s 8ms/step - loss: 0.2876 - acc: 0.9193 - val_loss: 0.5559 - val_acc: 0.8688\n",
      "Epoch 70/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2678 - acc: 0.928 - ETA: 3s - loss: 0.3044 - acc: 0.915 - ETA: 3s - loss: 0.2973 - acc: 0.917 - ETA: 2s - loss: 0.2999 - acc: 0.916 - ETA: 2s - loss: 0.2953 - acc: 0.917 - ETA: 2s - loss: 0.2889 - acc: 0.919 - ETA: 1s - loss: 0.2863 - acc: 0.920 - ETA: 1s - loss: 0.2856 - acc: 0.920 - ETA: 0s - loss: 0.2862 - acc: 0.919 - ETA: 0s - loss: 0.2851 - acc: 0.920 - 5s 8ms/step - loss: 0.2832 - acc: 0.9207 - val_loss: 0.5442 - val_acc: 0.8716\n",
      "Epoch 71/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2518 - acc: 0.929 - ETA: 4s - loss: 0.2640 - acc: 0.926 - ETA: 3s - loss: 0.2802 - acc: 0.922 - ETA: 3s - loss: 0.2786 - acc: 0.922 - ETA: 2s - loss: 0.2753 - acc: 0.924 - ETA: 2s - loss: 0.2778 - acc: 0.923 - ETA: 1s - loss: 0.2800 - acc: 0.922 - ETA: 1s - loss: 0.2812 - acc: 0.921 - ETA: 0s - loss: 0.2790 - acc: 0.921 - ETA: 0s - loss: 0.2785 - acc: 0.921 - 5s 8ms/step - loss: 0.2778 - acc: 0.9217 - val_loss: 0.5490 - val_acc: 0.8724\n",
      "Epoch 72/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2930 - acc: 0.918 - ETA: 3s - loss: 0.2838 - acc: 0.921 - ETA: 3s - loss: 0.2779 - acc: 0.923 - ETA: 2s - loss: 0.2734 - acc: 0.924 - ETA: 2s - loss: 0.2758 - acc: 0.923 - ETA: 1s - loss: 0.2740 - acc: 0.924 - ETA: 1s - loss: 0.2766 - acc: 0.923 - ETA: 1s - loss: 0.2747 - acc: 0.924 - ETA: 0s - loss: 0.2719 - acc: 0.925 - ETA: 0s - loss: 0.2749 - acc: 0.923 - 5s 8ms/step - loss: 0.2738 - acc: 0.9240 - val_loss: 0.5458 - val_acc: 0.8698\n",
      "Epoch 73/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2782 - acc: 0.920 - ETA: 3s - loss: 0.2843 - acc: 0.918 - ETA: 3s - loss: 0.2812 - acc: 0.919 - ETA: 2s - loss: 0.2729 - acc: 0.921 - ETA: 2s - loss: 0.2654 - acc: 0.925 - ETA: 1s - loss: 0.2624 - acc: 0.926 - ETA: 1s - loss: 0.2674 - acc: 0.925 - ETA: 1s - loss: 0.2694 - acc: 0.924 - ETA: 0s - loss: 0.2706 - acc: 0.923 - ETA: 0s - loss: 0.2720 - acc: 0.923 - 5s 8ms/step - loss: 0.2728 - acc: 0.9226 - val_loss: 0.5478 - val_acc: 0.8732\n",
      "Epoch 74/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2467 - acc: 0.929 - ETA: 3s - loss: 0.2610 - acc: 0.925 - ETA: 3s - loss: 0.2619 - acc: 0.924 - ETA: 2s - loss: 0.2652 - acc: 0.924 - ETA: 2s - loss: 0.2638 - acc: 0.924 - ETA: 1s - loss: 0.2646 - acc: 0.923 - ETA: 1s - loss: 0.2605 - acc: 0.925 - ETA: 1s - loss: 0.2615 - acc: 0.925 - ETA: 0s - loss: 0.2638 - acc: 0.925 - ETA: 0s - loss: 0.2670 - acc: 0.923 - 5s 8ms/step - loss: 0.2674 - acc: 0.9236 - val_loss: 0.5482 - val_acc: 0.8712\n",
      "Epoch 75/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2456 - acc: 0.933 - ETA: 3s - loss: 0.2674 - acc: 0.926 - ETA: 3s - loss: 0.2555 - acc: 0.929 - ETA: 2s - loss: 0.2639 - acc: 0.926 - ETA: 2s - loss: 0.2600 - acc: 0.926 - ETA: 2s - loss: 0.2581 - acc: 0.926 - ETA: 1s - loss: 0.2581 - acc: 0.927 - ETA: 1s - loss: 0.2593 - acc: 0.927 - ETA: 0s - loss: 0.2561 - acc: 0.928 - ETA: 0s - loss: 0.2579 - acc: 0.927 - 5s 8ms/step - loss: 0.2615 - acc: 0.9270 - val_loss: 0.5499 - val_acc: 0.8712\n",
      "Epoch 76/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2572 - acc: 0.924 - ETA: 3s - loss: 0.2666 - acc: 0.924 - ETA: 3s - loss: 0.2560 - acc: 0.928 - ETA: 2s - loss: 0.2607 - acc: 0.926 - ETA: 2s - loss: 0.2611 - acc: 0.926 - ETA: 1s - loss: 0.2613 - acc: 0.926 - ETA: 1s - loss: 0.2624 - acc: 0.926 - ETA: 1s - loss: 0.2599 - acc: 0.927 - ETA: 0s - loss: 0.2578 - acc: 0.927 - ETA: 0s - loss: 0.2565 - acc: 0.928 - 5s 8ms/step - loss: 0.2577 - acc: 0.9279 - val_loss: 0.5487 - val_acc: 0.8741\n",
      "Epoch 77/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2506 - acc: 0.930 - ETA: 3s - loss: 0.2774 - acc: 0.922 - ETA: 3s - loss: 0.2633 - acc: 0.926 - ETA: 2s - loss: 0.2568 - acc: 0.928 - ETA: 2s - loss: 0.2628 - acc: 0.926 - ETA: 1s - loss: 0.2574 - acc: 0.928 - ETA: 1s - loss: 0.2568 - acc: 0.928 - ETA: 1s - loss: 0.2574 - acc: 0.928 - ETA: 0s - loss: 0.2588 - acc: 0.927 - ETA: 0s - loss: 0.2575 - acc: 0.927 - 5s 8ms/step - loss: 0.2556 - acc: 0.9285 - val_loss: 0.5524 - val_acc: 0.8717\n",
      "Epoch 78/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2169 - acc: 0.943 - ETA: 3s - loss: 0.2256 - acc: 0.938 - ETA: 3s - loss: 0.2261 - acc: 0.937 - ETA: 2s - loss: 0.2387 - acc: 0.933 - ETA: 2s - loss: 0.2430 - acc: 0.931 - ETA: 1s - loss: 0.2461 - acc: 0.931 - ETA: 1s - loss: 0.2489 - acc: 0.929 - ETA: 1s - loss: 0.2462 - acc: 0.930 - ETA: 0s - loss: 0.2490 - acc: 0.929 - ETA: 0s - loss: 0.2500 - acc: 0.929 - 5s 8ms/step - loss: 0.2503 - acc: 0.9297 - val_loss: 0.5471 - val_acc: 0.8707\n",
      "Epoch 79/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2650 - acc: 0.923 - ETA: 3s - loss: 0.2558 - acc: 0.926 - ETA: 3s - loss: 0.2551 - acc: 0.927 - ETA: 2s - loss: 0.2532 - acc: 0.928 - ETA: 2s - loss: 0.2563 - acc: 0.927 - ETA: 1s - loss: 0.2542 - acc: 0.928 - ETA: 1s - loss: 0.2549 - acc: 0.928 - ETA: 1s - loss: 0.2514 - acc: 0.929 - ETA: 0s - loss: 0.2499 - acc: 0.929 - ETA: 0s - loss: 0.2504 - acc: 0.929 - 5s 8ms/step - loss: 0.2489 - acc: 0.9300 - val_loss: 0.5527 - val_acc: 0.8723\n",
      "Epoch 80/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2415 - acc: 0.934 - ETA: 3s - loss: 0.2420 - acc: 0.933 - ETA: 3s - loss: 0.2340 - acc: 0.935 - ETA: 2s - loss: 0.2448 - acc: 0.932 - ETA: 2s - loss: 0.2443 - acc: 0.932 - ETA: 1s - loss: 0.2443 - acc: 0.932 - ETA: 1s - loss: 0.2433 - acc: 0.932 - ETA: 1s - loss: 0.2482 - acc: 0.931 - ETA: 0s - loss: 0.2458 - acc: 0.931 - ETA: 0s - loss: 0.2476 - acc: 0.930 - 5s 8ms/step - loss: 0.2455 - acc: 0.9314 - val_loss: 0.5550 - val_acc: 0.8729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2461 - acc: 0.930 - ETA: 3s - loss: 0.2459 - acc: 0.930 - ETA: 3s - loss: 0.2327 - acc: 0.934 - ETA: 2s - loss: 0.2384 - acc: 0.932 - ETA: 2s - loss: 0.2410 - acc: 0.932 - ETA: 1s - loss: 0.2362 - acc: 0.933 - ETA: 1s - loss: 0.2398 - acc: 0.932 - ETA: 1s - loss: 0.2371 - acc: 0.933 - ETA: 0s - loss: 0.2357 - acc: 0.934 - ETA: 0s - loss: 0.2398 - acc: 0.932 - 5s 8ms/step - loss: 0.2419 - acc: 0.9324 - val_loss: 0.5509 - val_acc: 0.8731\n",
      "Epoch 82/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2622 - acc: 0.924 - ETA: 3s - loss: 0.2512 - acc: 0.930 - ETA: 3s - loss: 0.2475 - acc: 0.931 - ETA: 2s - loss: 0.2417 - acc: 0.931 - ETA: 2s - loss: 0.2489 - acc: 0.929 - ETA: 1s - loss: 0.2453 - acc: 0.930 - ETA: 1s - loss: 0.2436 - acc: 0.931 - ETA: 1s - loss: 0.2439 - acc: 0.931 - ETA: 0s - loss: 0.2413 - acc: 0.932 - ETA: 0s - loss: 0.2409 - acc: 0.932 - 5s 8ms/step - loss: 0.2403 - acc: 0.9323 - val_loss: 0.5458 - val_acc: 0.8747\n",
      "Epoch 83/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.2759 - acc: 0.920 - ETA: 3s - loss: 0.2636 - acc: 0.922 - ETA: 3s - loss: 0.2582 - acc: 0.924 - ETA: 2s - loss: 0.2474 - acc: 0.928 - ETA: 2s - loss: 0.2498 - acc: 0.927 - ETA: 1s - loss: 0.2482 - acc: 0.928 - ETA: 1s - loss: 0.2467 - acc: 0.929 - ETA: 1s - loss: 0.2440 - acc: 0.930 - ETA: 0s - loss: 0.2403 - acc: 0.931 - ETA: 0s - loss: 0.2381 - acc: 0.932 - 5s 8ms/step - loss: 0.2393 - acc: 0.9318 - val_loss: 0.5533 - val_acc: 0.8756\n",
      "Epoch 84/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2607 - acc: 0.923 - ETA: 3s - loss: 0.2345 - acc: 0.933 - ETA: 3s - loss: 0.2324 - acc: 0.933 - ETA: 2s - loss: 0.2313 - acc: 0.935 - ETA: 2s - loss: 0.2290 - acc: 0.936 - ETA: 1s - loss: 0.2327 - acc: 0.934 - ETA: 1s - loss: 0.2339 - acc: 0.934 - ETA: 1s - loss: 0.2346 - acc: 0.934 - ETA: 0s - loss: 0.2350 - acc: 0.934 - ETA: 0s - loss: 0.2360 - acc: 0.933 - 5s 8ms/step - loss: 0.2360 - acc: 0.9337 - val_loss: 0.5511 - val_acc: 0.8745\n",
      "Epoch 85/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2377 - acc: 0.932 - ETA: 3s - loss: 0.2121 - acc: 0.940 - ETA: 3s - loss: 0.2235 - acc: 0.937 - ETA: 2s - loss: 0.2215 - acc: 0.938 - ETA: 2s - loss: 0.2236 - acc: 0.937 - ETA: 1s - loss: 0.2303 - acc: 0.934 - ETA: 1s - loss: 0.2335 - acc: 0.933 - ETA: 1s - loss: 0.2342 - acc: 0.933 - ETA: 0s - loss: 0.2313 - acc: 0.934 - ETA: 0s - loss: 0.2331 - acc: 0.934 - 5s 8ms/step - loss: 0.2327 - acc: 0.9340 - val_loss: 0.5611 - val_acc: 0.8743\n",
      "Epoch 86/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.2281 - acc: 0.934 - ETA: 3s - loss: 0.2214 - acc: 0.937 - ETA: 3s - loss: 0.2213 - acc: 0.936 - ETA: 2s - loss: 0.2210 - acc: 0.937 - ETA: 2s - loss: 0.2234 - acc: 0.936 - ETA: 1s - loss: 0.2259 - acc: 0.936 - ETA: 1s - loss: 0.2232 - acc: 0.937 - ETA: 1s - loss: 0.2249 - acc: 0.936 - ETA: 0s - loss: 0.2250 - acc: 0.936 - ETA: 0s - loss: 0.2259 - acc: 0.936 - 5s 8ms/step - loss: 0.2285 - acc: 0.9352 - val_loss: 0.5609 - val_acc: 0.8710\n",
      "Epoch 87/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.2570 - acc: 0.930 - ETA: 3s - loss: 0.2478 - acc: 0.930 - ETA: 3s - loss: 0.2375 - acc: 0.934 - ETA: 2s - loss: 0.2285 - acc: 0.936 - ETA: 2s - loss: 0.2332 - acc: 0.934 - ETA: 1s - loss: 0.2276 - acc: 0.936 - ETA: 1s - loss: 0.2252 - acc: 0.937 - ETA: 1s - loss: 0.2262 - acc: 0.936 - ETA: 0s - loss: 0.2254 - acc: 0.936 - ETA: 0s - loss: 0.2298 - acc: 0.934 - 5s 7ms/step - loss: 0.2276 - acc: 0.9355 - val_loss: 0.5507 - val_acc: 0.8773\n",
      "Epoch 88/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2259 - acc: 0.930 - ETA: 3s - loss: 0.2344 - acc: 0.930 - ETA: 3s - loss: 0.2267 - acc: 0.934 - ETA: 2s - loss: 0.2316 - acc: 0.932 - ETA: 2s - loss: 0.2275 - acc: 0.933 - ETA: 1s - loss: 0.2247 - acc: 0.934 - ETA: 1s - loss: 0.2230 - acc: 0.935 - ETA: 1s - loss: 0.2242 - acc: 0.935 - ETA: 0s - loss: 0.2210 - acc: 0.936 - ETA: 0s - loss: 0.2206 - acc: 0.936 - 5s 8ms/step - loss: 0.2206 - acc: 0.9364 - val_loss: 0.5518 - val_acc: 0.8761\n",
      "Epoch 89/100\n",
      "664/664 [==============================] - ETA: 5s - loss: 0.2539 - acc: 0.927 - ETA: 4s - loss: 0.2385 - acc: 0.932 - ETA: 3s - loss: 0.2273 - acc: 0.935 - ETA: 3s - loss: 0.2243 - acc: 0.935 - ETA: 2s - loss: 0.2244 - acc: 0.935 - ETA: 2s - loss: 0.2234 - acc: 0.935 - ETA: 1s - loss: 0.2227 - acc: 0.936 - ETA: 1s - loss: 0.2183 - acc: 0.937 - ETA: 0s - loss: 0.2196 - acc: 0.937 - ETA: 0s - loss: 0.2194 - acc: 0.937 - 5s 8ms/step - loss: 0.2180 - acc: 0.9379 - val_loss: 0.5507 - val_acc: 0.8770\n",
      "Epoch 90/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2078 - acc: 0.941 - ETA: 4s - loss: 0.2091 - acc: 0.941 - ETA: 3s - loss: 0.2116 - acc: 0.940 - ETA: 2s - loss: 0.2167 - acc: 0.937 - ETA: 2s - loss: 0.2171 - acc: 0.938 - ETA: 2s - loss: 0.2172 - acc: 0.937 - ETA: 1s - loss: 0.2148 - acc: 0.938 - ETA: 1s - loss: 0.2103 - acc: 0.939 - ETA: 0s - loss: 0.2125 - acc: 0.939 - ETA: 0s - loss: 0.2134 - acc: 0.939 - 5s 8ms/step - loss: 0.2144 - acc: 0.9388 - val_loss: 0.5565 - val_acc: 0.8745\n",
      "Epoch 91/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2027 - acc: 0.943 - ETA: 3s - loss: 0.2120 - acc: 0.940 - ETA: 3s - loss: 0.2143 - acc: 0.939 - ETA: 2s - loss: 0.2060 - acc: 0.942 - ETA: 2s - loss: 0.2065 - acc: 0.942 - ETA: 1s - loss: 0.2102 - acc: 0.940 - ETA: 1s - loss: 0.2119 - acc: 0.939 - ETA: 1s - loss: 0.2134 - acc: 0.939 - ETA: 0s - loss: 0.2110 - acc: 0.940 - ETA: 0s - loss: 0.2113 - acc: 0.940 - 5s 7ms/step - loss: 0.2129 - acc: 0.9397 - val_loss: 0.5585 - val_acc: 0.8750\n",
      "Epoch 92/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.1996 - acc: 0.940 - ETA: 3s - loss: 0.1991 - acc: 0.943 - ETA: 3s - loss: 0.2041 - acc: 0.942 - ETA: 2s - loss: 0.2018 - acc: 0.942 - ETA: 2s - loss: 0.2094 - acc: 0.940 - ETA: 1s - loss: 0.2157 - acc: 0.937 - ETA: 1s - loss: 0.2179 - acc: 0.937 - ETA: 1s - loss: 0.2146 - acc: 0.938 - ETA: 0s - loss: 0.2131 - acc: 0.939 - ETA: 0s - loss: 0.2129 - acc: 0.939 - 5s 8ms/step - loss: 0.2131 - acc: 0.9392 - val_loss: 0.5567 - val_acc: 0.8737\n",
      "Epoch 93/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.1943 - acc: 0.947 - ETA: 3s - loss: 0.2002 - acc: 0.944 - ETA: 3s - loss: 0.2094 - acc: 0.941 - ETA: 2s - loss: 0.2058 - acc: 0.942 - ETA: 2s - loss: 0.2075 - acc: 0.941 - ETA: 1s - loss: 0.2054 - acc: 0.941 - ETA: 1s - loss: 0.2073 - acc: 0.941 - ETA: 1s - loss: 0.2109 - acc: 0.939 - ETA: 0s - loss: 0.2122 - acc: 0.939 - ETA: 0s - loss: 0.2106 - acc: 0.939 - 5s 8ms/step - loss: 0.2089 - acc: 0.9400 - val_loss: 0.5582 - val_acc: 0.8775\n",
      "Epoch 94/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.2104 - acc: 0.944 - ETA: 3s - loss: 0.2077 - acc: 0.942 - ETA: 3s - loss: 0.2105 - acc: 0.940 - ETA: 2s - loss: 0.2085 - acc: 0.940 - ETA: 2s - loss: 0.2086 - acc: 0.940 - ETA: 1s - loss: 0.2052 - acc: 0.941 - ETA: 1s - loss: 0.2069 - acc: 0.941 - ETA: 1s - loss: 0.2066 - acc: 0.941 - ETA: 0s - loss: 0.2062 - acc: 0.941 - ETA: 0s - loss: 0.2058 - acc: 0.941 - 5s 7ms/step - loss: 0.2057 - acc: 0.9415 - val_loss: 0.5598 - val_acc: 0.8739\n",
      "Epoch 95/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.1822 - acc: 0.948 - ETA: 3s - loss: 0.1900 - acc: 0.946 - ETA: 3s - loss: 0.1844 - acc: 0.948 - ETA: 2s - loss: 0.1917 - acc: 0.946 - ETA: 2s - loss: 0.1914 - acc: 0.946 - ETA: 1s - loss: 0.1989 - acc: 0.943 - ETA: 1s - loss: 0.2008 - acc: 0.942 - ETA: 1s - loss: 0.2012 - acc: 0.942 - ETA: 0s - loss: 0.2004 - acc: 0.942 - ETA: 0s - loss: 0.2025 - acc: 0.941 - 5s 7ms/step - loss: 0.2026 - acc: 0.9418 - val_loss: 0.5509 - val_acc: 0.8764\n",
      "Epoch 96/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.1804 - acc: 0.949 - ETA: 3s - loss: 0.1803 - acc: 0.948 - ETA: 3s - loss: 0.1847 - acc: 0.947 - ETA: 2s - loss: 0.1902 - acc: 0.945 - ETA: 2s - loss: 0.1981 - acc: 0.942 - ETA: 1s - loss: 0.1989 - acc: 0.942 - ETA: 1s - loss: 0.1989 - acc: 0.943 - ETA: 1s - loss: 0.1973 - acc: 0.943 - ETA: 0s - loss: 0.1977 - acc: 0.944 - ETA: 0s - loss: 0.1996 - acc: 0.943 - 5s 7ms/step - loss: 0.1994 - acc: 0.9434 - val_loss: 0.5567 - val_acc: 0.8772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.1890 - acc: 0.946 - ETA: 3s - loss: 0.1936 - acc: 0.944 - ETA: 3s - loss: 0.1902 - acc: 0.945 - ETA: 2s - loss: 0.1907 - acc: 0.944 - ETA: 2s - loss: 0.1950 - acc: 0.944 - ETA: 1s - loss: 0.1928 - acc: 0.944 - ETA: 1s - loss: 0.1915 - acc: 0.945 - ETA: 1s - loss: 0.1896 - acc: 0.945 - ETA: 0s - loss: 0.1933 - acc: 0.944 - ETA: 0s - loss: 0.1968 - acc: 0.943 - 5s 7ms/step - loss: 0.1973 - acc: 0.9431 - val_loss: 0.5525 - val_acc: 0.8764\n",
      "Epoch 98/100\n",
      "664/664 [==============================] - ETA: 3s - loss: 0.2099 - acc: 0.937 - ETA: 3s - loss: 0.2044 - acc: 0.941 - ETA: 3s - loss: 0.1911 - acc: 0.946 - ETA: 2s - loss: 0.1882 - acc: 0.947 - ETA: 2s - loss: 0.1899 - acc: 0.945 - ETA: 1s - loss: 0.1912 - acc: 0.944 - ETA: 1s - loss: 0.1876 - acc: 0.946 - ETA: 1s - loss: 0.1883 - acc: 0.945 - ETA: 0s - loss: 0.1909 - acc: 0.944 - ETA: 0s - loss: 0.1933 - acc: 0.944 - 5s 7ms/step - loss: 0.1937 - acc: 0.9443 - val_loss: 0.5551 - val_acc: 0.8761\n",
      "Epoch 99/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.1842 - acc: 0.947 - ETA: 3s - loss: 0.1918 - acc: 0.945 - ETA: 3s - loss: 0.1843 - acc: 0.948 - ETA: 2s - loss: 0.1888 - acc: 0.946 - ETA: 2s - loss: 0.1876 - acc: 0.945 - ETA: 1s - loss: 0.1861 - acc: 0.946 - ETA: 1s - loss: 0.1870 - acc: 0.946 - ETA: 1s - loss: 0.1880 - acc: 0.946 - ETA: 0s - loss: 0.1895 - acc: 0.946 - ETA: 0s - loss: 0.1891 - acc: 0.946 - 5s 8ms/step - loss: 0.1903 - acc: 0.9461 - val_loss: 0.5584 - val_acc: 0.8777\n",
      "Epoch 100/100\n",
      "664/664 [==============================] - ETA: 4s - loss: 0.1928 - acc: 0.943 - ETA: 3s - loss: 0.1881 - acc: 0.944 - ETA: 3s - loss: 0.1875 - acc: 0.944 - ETA: 2s - loss: 0.1957 - acc: 0.943 - ETA: 2s - loss: 0.1957 - acc: 0.943 - ETA: 1s - loss: 0.1943 - acc: 0.944 - ETA: 1s - loss: 0.1937 - acc: 0.944 - ETA: 1s - loss: 0.1915 - acc: 0.945 - ETA: 0s - loss: 0.1893 - acc: 0.946 - ETA: 0s - loss: 0.1873 - acc: 0.946 - 5s 7ms/step - loss: 0.1891 - acc: 0.9461 - val_loss: 0.5603 - val_acc: 0.8755\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2,\n",
    "  verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4BYRyTMKbhGRPKPgFhHJM/8f0ghwJirWTZEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21f81c342e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJOEuhBBeSMJdCCG8kIS7EEJ4of8P8z/e8fI+RLIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21f81c7eba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model\n",
    "# model.save('...h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Make predictions #####\n",
    "# We need to create another model\n",
    "# that can take in the RNN state and previous word as input\n",
    "# and accept a T=1 sequence.\n",
    "\n",
    "# The encoder will be stand-alone. It is exactly as the encoder in the training model\n",
    "# From this we will get our initial decoder hidden state (h,c)\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These inputs are placeholders for the output of the encoder_model above\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,)) \n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,)) \n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_states_inputs = [decoder_state_input_h] # gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "# The single word input (at its time) will be embedded\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this time, we want to keep the states too (h and c of the decoder), to be output\n",
    "# by our sampling model\n",
    "decoder_outputs, h, c = decoder_lstm(\n",
    "  decoder_inputs_single_x, # This is the input of the generated previous word in the prediction\n",
    "  initial_state=decoder_states_inputs # The h and c states from encoder_model\n",
    ")\n",
    "# decoder_outputs, state_h = decoder_lstm(\n",
    "#   decoder_inputs_single_x,\n",
    "#   initial_state=decoder_states_inputs\n",
    "# ) #gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_states = [h, c]\n",
    "# decoder_states = [h] # gru\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The sampling model\n",
    "# inputs: y(t-1), h(t-1), c(t-1)\n",
    "# outputs: y(t), h(t), c(t)\n",
    "decoder_model = Model(\n",
    "  [decoder_inputs_single] + decoder_states_inputs, \n",
    "  [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map indexes back into real words\n",
    "# so we can view the results (it's the opposite, v:k instead k:v)\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in nltk_word_index.items()} # nltk_word_index replaced word2idx_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # NOTE: tokenizer lower-cases all words\n",
    "    target_seq[0, 0] = nltk_word_index['<'] # Instead of <sos> till parsing is better (as input token). First Token\n",
    "\n",
    "    # if we get this we break\n",
    "    eos = nltk_word_index['>'] # Instead of <eos> till parsing is better (as input token). Last Token\n",
    "\n",
    "    # Create the translation - make prediction 7 (or any target length) times, and each time generate token, h, c\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        # output_tokens, h = decoder_model.predict([target_seq] + states_value) # gru\n",
    "\n",
    "        # Get next word\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        # Update the decoder input\n",
    "        # which is just the word just generated\n",
    "        target_seq[0, 0] = idx\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        # states_value = [h] # gru\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   7 43  8 16  4 33  6  2 85]]\n",
      "-\n",
      "Input: find 2 consecutive odd integers whose sum is 56\n",
      "Translation: ['( 2 0 k 1 0 1 + 0 2 0 2 * + . )= . * . - . * . 1 . 1 = 0 ']\n",
      "Continue? [Y/n]Y\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  6\n",
      "   3 40  8  4  2 79  7  1  4]]\n",
      "-\n",
      "Input: the sum of five consecutive integers is 0 . find the integers ?\n",
      "Translation: ['( x - . )+ . +( - . * - . )+( . * + )= . )= . < eos 0 eos <\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Do some test translations\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    print(input_seq)\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', input_texts[i])\n",
    "    print('Translation:', translation)\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = [\"Two numbers have a difference of 10 and the sum of 34 . What are the numbers ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 12, 63, 23, 39, 3, 58, 15, 1, 6, 3, 120, 14, 25, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "example_sequences = tokenizer_inputs.texts_to_sequences(example)\n",
    "print(example_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "   12  63  23  39   3  58  15   1   6   3 120  14  25   1  12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[' x y - = 0 . ', x y x y x 5 0 5 0 x y eos < ']\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inputs = pad_sequences(example_sequences, maxlen=max_len_input)\n",
    "print(example_inputs)\n",
    "example_translation = decode_sequence(example_inputs)\n",
    "example_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
